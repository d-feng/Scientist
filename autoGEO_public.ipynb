{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e8a1726-9e8e-4087-9e48-1b7f69665b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import gzip\n",
    "import shutil\n",
    "import tarfile\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import TypedDict, List, Dict, Any\n",
    "from urllib.parse import urljoin, urlparse, parse_qs, unquote\n",
    "\n",
    "# Third-Party Libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from openai import OpenAI\n",
    "from IPython.display import display, Markdown\n",
    "from geofetch import Geofetcher\n",
    "\n",
    "# Bioinformatics\n",
    "from pydeseq2.dds import DeseqDataSet\n",
    "from pydeseq2.ds import DeseqStats\n",
    "from pydeseq2.default_inference import DefaultInference\n",
    "import gseapy as gp\n",
    "# LangChain/LangGraph\n",
    "from langgraph.graph import StateGraph\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "API_KEY = ''\n",
    "os.environ['OPENAI_API_KEY']=API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57ede832-246b-46bd-bc0d-ef48324e1c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client = OpenAI()\n",
    "class AgentState(TypedDict):\n",
    "    query_gene: str\n",
    "    pathway_info: Dict[str, Any]\n",
    "    metadata: Dict[str, Any]\n",
    "    gse_list: list\n",
    "    research_plan: str\n",
    "    selected_gse: str  # ‚¨ÖÔ∏è now added\n",
    "    selected_gses: List[str]  # ‚úÖ new field for multiple GSEs\n",
    "    deg_results: Dict[str, str]  # ‚úÖ map GSE ‚Üí output CSV path\n",
    "\n",
    "# Utility to extract all GSE IDs from plan\n",
    "def extract_all_gse_ids(plan_text: str) -> List[str]:\n",
    "    return re.findall(r\"GSE\\d{5,}\", plan_text)\n",
    "\n",
    "def extract_gse_from_research_plan(state: AgentState) -> AgentState:\n",
    "    plan = state.get(\"research_plan\", \"\")\n",
    "    match = re.search(r\"GSE\\d{5,}\", plan)\n",
    "    if match:\n",
    "        selected_gse = match.group(0)\n",
    "        print(f\"üîç Extracted GSE ID: {selected_gse}\")\n",
    "        return {**state, \"selected_gse\": selected_gse}\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No GSE ID found in research plan.\")\n",
    "        return state\n",
    "\n",
    "# LangGraph node: Extract GSEs from research plan and update state\n",
    "def extract_gse_list_from_plan(state: AgentState) -> AgentState:\n",
    "    plan = state.get(\"research_plan\", \"\")\n",
    "    gse_ids = extract_all_gse_ids(plan)\n",
    "    print(f\"üîç Extracted GSE IDs: {gse_ids}\")\n",
    "    return {**state, \"selected_gses\": gse_ids}\n",
    "\n",
    "# ----------------------------------------\n",
    "# Utility functions (LLM + GEO search)\n",
    "# ----------------------------------------\n",
    "\n",
    "def extract_json_block(text):\n",
    "    match = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", text, re.DOTALL)\n",
    "    if match:\n",
    "        return json.loads(match.group(1))\n",
    "    else:\n",
    "        return json.loads(text)\n",
    "\n",
    "\n",
    "def extract_json_block(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extract and parse the first JSON block from LLM output, with fallback and error reporting.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import re\n",
    "\n",
    "    try:\n",
    "        match = re.search(r\"```json\\s*({.*?})\\s*```\", text, re.DOTALL)\n",
    "        if match:\n",
    "            return json.loads(match.group(1))\n",
    "        return json.loads(text)  # fallback if no block match\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"‚ö†Ô∏è JSON decode error:\", e)\n",
    "        print(\"üîç Raw content:\\n\", text[:1000])  # preview\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_pathway_info(query_gene, model=\"gpt-4o-mini\", temperature=0.3):\n",
    "    prompt = f\"\"\"\n",
    "You are a biomedical assistant.\n",
    "\n",
    "Given the gene {query_gene}, return:\n",
    "1. Key gene symbols in the same biological pathway.\n",
    "2. Drugs or compounds that inhibit this pathway or {query_gene}'s activity.\n",
    "3. The disease areas or biological processes this pathway is involved in.\n",
    "\n",
    "Provide answers in JSON format with keys: \"genes\", \"drugs\", \"pathways\"\n",
    "    \"\"\"\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    content = completion.choices[0].message.content\n",
    "    return extract_json_block(content)\n",
    "\n",
    "\n",
    "\n",
    "def scrape_organism_from_geo_html(geo_accession):\n",
    "    url = f\"https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc={geo_accession}\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        organism_row = soup.find(\"td\", string=\"Organism\")\n",
    "        if organism_row and organism_row.find_next_sibling(\"td\"):\n",
    "            return organism_row.find_next_sibling(\"td\").text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to scrape organism for {geo_accession}: {e}\")\n",
    "    return \"Unknown\"\n",
    "\n",
    "def search_geo_datasets(keyword, retmax=10):\n",
    "    base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "    params = {\n",
    "        \"db\": \"gds\",\n",
    "        \"term\": keyword,\n",
    "        \"retmode\": \"json\",\n",
    "        \"retmax\": retmax\n",
    "    }\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()\n",
    "    return response.json().get(\"esearchresult\", {}).get(\"idlist\", [])\n",
    "\n",
    "def fetch_gse_accessions(id_list):\n",
    "    if not id_list:\n",
    "        return {}\n",
    "    base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi\"\n",
    "    params = {\n",
    "        \"db\": \"gds\",\n",
    "        \"id\": \",\".join(id_list),\n",
    "        \"retmode\": \"json\"\n",
    "    }\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()\n",
    "    summaries = response.json().get(\"result\", {})\n",
    "    summaries.pop(\"uids\", None)\n",
    "    gse_dict = {}\n",
    "    for uid, info in summaries.items():\n",
    "        accession = info.get(\"accession\")\n",
    "        title = info.get(\"title\")\n",
    "        link = f\"https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc={accession}\"\n",
    "        organism = scrape_organism_from_geo_html(accession)\n",
    "        gse_dict[accession] = {\n",
    "            \"title\": title,\n",
    "            \"organism\": organism,\n",
    "            \"link\": link\n",
    "        }\n",
    "    return gse_dict\n",
    "\n",
    "def search_geo_items(item_list, label=\"gene\", max_results=5):\n",
    "    results = {}\n",
    "    for item in item_list:\n",
    "        query = f\"{item} AND rna-seq\"\n",
    "        print(f\"üîç Searching GEO for {label}: {item}\")\n",
    "        ids = search_geo_datasets(query, retmax=max_results)\n",
    "        datasets = fetch_gse_accessions(ids)\n",
    "        results[item] = datasets\n",
    "        time.sleep(0.3)\n",
    "    return results\n",
    "\n",
    "def get_geofetch_projects(gse_list, metadata_folder=\"geofetch_metadata\"):\n",
    "    geof = Geofetcher(\n",
    "        processed=True,\n",
    "        acc_anno=True,\n",
    "        discard_soft=True,\n",
    "        metadata_folder=metadata_folder\n",
    "    )\n",
    "    projects = {}\n",
    "    for gse in gse_list:\n",
    "        try:\n",
    "            print(f\"üì• Fetching metadata for {gse}\")\n",
    "            result = geof.get_projects(gse)\n",
    "            projects.update(result)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to fetch {gse}: {e}\")\n",
    "    return projects\n",
    "\n",
    "import subprocess\n",
    "def download_processed_files_via_cli(gse_list, output_dir=\"geofetch_metadata\", overwrite=False):\n",
    "    for gse in gse_list:\n",
    "        gse_path = os.path.join(output_dir, gse)\n",
    "        if os.path.exists(gse_path) and not overwrite:\n",
    "            print(f\"‚úÖ {gse}: already exists at {gse_path}, skipping.\")\n",
    "            continue\n",
    "        try:\n",
    "            subprocess.run(\n",
    "                [\"geofetch\", \"-i\", gse, \"--processed\", \"-m\", output_dir],\n",
    "                check=True\n",
    "            )\n",
    "            print(f\"‚úÖ Finished downloading for {gse}\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ùå geofetch failed for {gse}: {e}\")\n",
    "\n",
    "def save_combined_metadata_csv_from_state(state: Dict, csv_path: str = \"geofetch_metadata/combined_metadata.csv\") -> pd.DataFrame:\n",
    "    metadata = state.get(\"metadata\", {})\n",
    "    if not metadata:\n",
    "        raise ValueError(\"No metadata found in the agent state.\")\n",
    "\n",
    "    all_dfs = []\n",
    "    for gse, project in metadata.items():\n",
    "        try:\n",
    "            df = project.sample_table.copy()\n",
    "            df[\"source_gse\"] = gse\n",
    "            all_dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to extract sample_table from {gse}: {e}\")\n",
    "\n",
    "    if not all_dfs:\n",
    "        raise ValueError(\"No sample tables to save.\")\n",
    "\n",
    "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
    "    combined_df.to_csv(csv_path, index=False)\n",
    "    print(f\"‚úÖ Combined metadata saved to: {os.path.abspath(csv_path)}\")\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def scrape_geo_supplementary_downloads(gse_id: str):\n",
    "    \"\"\"\n",
    "    Scrape the GEO page for a given GSE ID and return a list of supplementary\n",
    "    files that are .tar, .gz, or .tar.gz.\n",
    "\n",
    "    Returns:\n",
    "        List of tuples: (file_name, download_url)\n",
    "    \"\"\"\n",
    "    base_url = \"https://www.ncbi.nlm.nih.gov\"\n",
    "    url = f\"{base_url}/geo/query/acc.cgi?acc={gse_id}\"\n",
    "    print(f\"üîç Scraping GEO page: {url}\")\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        print(\"‚úÖ Page fetched successfully\")\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        tables = soup.find_all(\"table\")\n",
    "        print(f\"üîé Found {len(tables)} total tables\")\n",
    "\n",
    "        for idx, table in enumerate(tables):\n",
    "            if \"Supplementary file\" in table.text:\n",
    "                print(f\"üìã Found supplementary file table at index {idx}\")\n",
    "                rows = table.find_all(\"tr\")\n",
    "                print(f\"üìÑ Table has {len(rows) - 1} data rows\")\n",
    "\n",
    "                files = []\n",
    "\n",
    "                for row in rows[1:]:  # skip header\n",
    "                    cols = row.find_all(\"td\")\n",
    "                    if len(cols) >= 3:\n",
    "                        file_name = cols[0].text.strip()\n",
    "                        link_tag = cols[2].find(\"a\", href=True)\n",
    "\n",
    "                        if link_tag:\n",
    "                            href = link_tag[\"href\"]\n",
    "\n",
    "                            # Only include valid file extensions\n",
    "                            if file_name.endswith((\".tar\", \".gz\", \".tar.gz\")):\n",
    "                                if href.startswith(\"/geo/download\"):\n",
    "                                    full_link = urljoin(base_url, href)\n",
    "                                    files.append((file_name, full_link))\n",
    "                                elif href.startswith(\"ftp://\") or href.startswith(\"http://\") or href.startswith(\"https://\"):\n",
    "                                    files.append((file_name, href))\n",
    "\n",
    "                if files:\n",
    "                    return files\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è Found table, but no matching .tar/.gz files.\")\n",
    "                    return []\n",
    "\n",
    "        print(\"‚ö†Ô∏è No supplementary file table found.\")\n",
    "        return []\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def ingest_and_prepare(state: dict) -> dict:\n",
    "    query_gene = state[\"query_gene\"]\n",
    "    pathway_info = generate_pathway_info(query_gene)\n",
    "    gene_list = pathway_info.get(\"genes\", [])\n",
    "    drug_list = pathway_info.get(\"drugs\", [])\n",
    "\n",
    "    gene_results = search_geo_items(gene_list, label=\"gene\")\n",
    "    drug_results = search_geo_items(drug_list, label=\"drug\")\n",
    "\n",
    "    all_gse = set()\n",
    "    for r in [gene_results, drug_results]:\n",
    "        for v in r.values():\n",
    "            all_gse.update(v.keys())\n",
    "    all_gse = list(all_gse)\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # Download each GSE using embedded logic\n",
    "    # ----------------------------------------\n",
    "    base_dir = Path(\"rna_seq_analysis\")\n",
    "    for gse_id in all_gse:\n",
    "        download_dir = base_dir / gse_id\n",
    "\n",
    "        if download_dir.exists() and any(download_dir.iterdir()):\n",
    "            print(f\"üìÅ Skipping {gse_id}: folder already exists.\")\n",
    "            continue\n",
    "\n",
    "        download_dir.mkdir(parents=True, exist_ok=True)\n",
    "        files = scrape_geo_supplementary_downloads(gse_id)\n",
    "        if not files:\n",
    "            print(f\"‚ö†Ô∏è No supplementary files found for {gse_id}\")\n",
    "            continue\n",
    "\n",
    "        file_name, url = files[0]\n",
    "        if not file_name:\n",
    "            parsed = urlparse(url)\n",
    "            qs = parse_qs(parsed.query)\n",
    "            file_name = unquote(qs.get(\"file\", [\"unnamed_file\"])[0])\n",
    "\n",
    "        file_path = download_dir / file_name\n",
    "        print(f\"üì• Downloading {file_name} ‚Üí {file_path.as_posix()}\")\n",
    "\n",
    "        try:\n",
    "            subprocess.run([\"curl\", \"-L\", \"-o\", file_path.as_posix(), url], check=True)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ùå Download failed for {gse_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if tarfile.is_tarfile(file_path):\n",
    "                print(\"üóÇÔ∏è Extracting TAR...\")\n",
    "                with tarfile.open(file_path, \"r:*\") as tar:\n",
    "                    tar.extractall(path=download_dir)\n",
    "            elif file_path.suffix == \".gz\" and not file_path.name.endswith(\".tar.gz\"):\n",
    "                unzipped_path = file_path.with_suffix(\"\")\n",
    "                print(\"üóÇÔ∏è Extracting GZ...\")\n",
    "                with gzip.open(file_path, 'rb') as f_in:\n",
    "                    with open(unzipped_path, 'wb') as f_out:\n",
    "                        shutil.copyfileobj(f_in, f_out)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Extraction failed for {gse_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # Continue with metadata and return\n",
    "    # ----------------------------------------\n",
    "    metadata = get_geofetch_projects(all_gse, metadata_folder=\"geofetch_metadata\")\n",
    "    download_processed_files_via_cli(all_gse, output_dir=\"geofetch_metadata\")\n",
    "\n",
    "    try:\n",
    "        save_combined_metadata_csv_from_state({\"metadata\": metadata})\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to save metadata CSV: {e}\")\n",
    "\n",
    "    return {\n",
    "        \"query_gene\": query_gene,\n",
    "        \"pathway_info\": pathway_info,\n",
    "        \"metadata\": metadata,\n",
    "        \"gse_list\": all_gse\n",
    "    }\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# Agent 2: Analyst node\n",
    "# ----------------------------------------\n",
    "\n",
    "def analyze_metadata_and_plan(state: AgentState) -> AgentState:\n",
    "    metadata = state[\"metadata\"]\n",
    "    drug_list = state[\"pathway_info\"].get(\"drugs\", [])\n",
    "    query_gene = state[\"query_gene\"]\n",
    "    selected = []\n",
    "\n",
    "    for gse, project in metadata.items():\n",
    "        df = project.sample_table\n",
    "        if \"processed_file_ftp\" in df.columns and df[\"processed_file_ftp\"].notna().any():\n",
    "            if any(drug.lower() in df.to_string().lower() for drug in drug_list):\n",
    "                selected.append((gse, df.shape[0]))\n",
    "\n",
    "    plan = f\"üß¨ Research Plan for {query_gene} and drugs {drug_list}:\\n\"\n",
    "    if not selected:\n",
    "        plan += \"No relevant processed datasets were found.\\n\"\n",
    "    else:\n",
    "        plan += f\"{len(selected)} datasets selected:\\n\"\n",
    "        for gse, n in selected:\n",
    "            plan += f\"  - {gse} ({n} samples)\\n\"\n",
    "        plan += \"\\nNext: perform differential expression and gene signature clustering.\"\n",
    "\n",
    "    return {**state, \"research_plan\": plan}\n",
    "\n",
    "\n",
    "def analyze_metadata_and_plan(state: AgentState) -> AgentState:\n",
    "    import pandas as pd\n",
    "    import os\n",
    "\n",
    "    csv_path = \"geofetch_metadata/combined_metadata.csv\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"‚ùå Metadata CSV not found at {csv_path}\")\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Ensure required columns exist\n",
    "    required_columns = {\"sample_name\", \"sample_source_name_ch1\", \"sample_title\"}\n",
    "    if not required_columns.issubset(df.columns):\n",
    "        raise ValueError(f\"‚ùå Metadata CSV must include columns: {required_columns}\")\n",
    "\n",
    "    # Take first 50 rows for LLM context\n",
    "    selected_df = df[list(required_columns)].fillna(\"\").head(50)\n",
    "    table_preview = selected_df.to_markdown(index=False)\n",
    "\n",
    "    query_gene = state[\"query_gene\"]\n",
    "    drug_list = state[\"pathway_info\"].get(\"drugs\", [])\n",
    "    selected_df = df[[\"sample_name\", \"sample_source_name_ch1\", \"sample_title\"]].fillna(\"\").head(50)\n",
    "    table_preview = selected_df.to_markdown(index=False)\n",
    "    print(\"üß™ Table preview sent to LLM:\\n\", table_preview)  # ‚úÖ Add this line\n",
    "\n",
    "    # Build LLM prompt\n",
    "    prompt = f\"\"\"\n",
    "You are a biomedical research assistant.\n",
    "\n",
    "The target gene is **{query_gene}**, and the related drugs of interest are: {', '.join(drug_list)}.\n",
    "\n",
    "Below is a preview of sample metadata (first 50 rows) from multiple GEO studies. Each row includes:\n",
    "- sample name\n",
    "- sample source (cell line, tissue)\n",
    "- sample title (may indicate treatment or condition)\n",
    "\n",
    "Your task:\n",
    "1. Identify which studies include drug-treated samples.\n",
    "2. Identify the control groups if available.\n",
    "3. Determine the sample types (e.g., cell lines or tissues).\n",
    "4. Recommend studies and sample comparisons suitable for differential gene expression and drug-response signature analysis.\n",
    "\n",
    "Respond with:\n",
    "- GSE or study names (if known)\n",
    "- The experimental comparison design\n",
    "- Why the dataset is suitable (or not)\n",
    "- Bullet points summarizing each recommended comparison\n",
    "\n",
    "Sample Metadata Table:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # Call LLM\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0.3,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    research_plan = response.choices[0].message.content.strip()\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"research_plan\": research_plan\n",
    "    }\n",
    "\n",
    "###\n",
    "def analyze_metadata_and_plan(state: AgentState) -> AgentState:\n",
    "    import pandas as pd\n",
    "    import os\n",
    "\n",
    "    csv_path = \"geofetch_metadata/combined_metadata.csv\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"‚ùå Metadata CSV not found at {csv_path}\")\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Ensure required columns are available\n",
    "    required_cols = [\"gse\", \"sample_name\", \"sample_title\", \"sample_source_name_ch1\", \"sample_geo_accession\"]\n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        raise ValueError(f\"‚ùå Metadata CSV must contain the following columns: {required_cols}\")\n",
    "\n",
    "    # Clean and preview first 30 rows\n",
    "    #preview_df = df[required_cols].fillna(\"\").head(30)\n",
    "    preview_df = df[required_cols].fillna(\"\")\n",
    "    preview_text = preview_df.to_string(index=False)\n",
    "\n",
    "    query_gene = state[\"query_gene\"]\n",
    "    drug_list = state[\"pathway_info\"].get(\"drugs\", [])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a biomedical research assistant.\n",
    "\n",
    "The target gene is **{query_gene}** and the related drugs of interest are: {', '.join(drug_list)}.\n",
    "\n",
    "Below is a preview of sample metadata from several GEO datasets.\n",
    "Each row includes:\n",
    "- GSE accession\n",
    "- Sample name\n",
    "- Sample title (may contain treatment or control info)\n",
    "- Sample source (cell type or tissue)\n",
    "- Sample GEO accession\n",
    "\n",
    "Sample Metadata Table:\n",
    "{preview_text}\n",
    "\n",
    "Based on the sample names, titles, and sources:\n",
    "1. Which GSE studies contain drug-treated samples and matching control groups?\n",
    "2. What cell types or tissues are used?\n",
    "3. Which treatments are applied? What are the controls?\n",
    "4. Recommend GSEs and sample pairs suitable for differential gene expression to identify drug-response gene signatures.\n",
    "\n",
    "Be specific, refer to GSE and sample names where possible, and explain why you recommend them.\n",
    "\"\"\"\n",
    "\n",
    "    # Call OpenAI LLM\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0.3,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    research_plan = response.choices[0].message.content.strip()\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"research_plan\": research_plan\n",
    "    }\n",
    "\n",
    "\n",
    "# LangGraph node: Extract GSEs from research plan and update state\n",
    "# Utility to extract all GSE IDs from plan\n",
    "def extract_all_gse_ids(plan_text: str) -> List[str]:\n",
    "    return re.findall(r\"GSE\\d{5,}\", plan_text)\n",
    "def extract_gse_list_from_plan(state: AgentState) -> AgentState:\n",
    "    plan = state.get(\"research_plan\", \"\")\n",
    "    gse_ids = extract_all_gse_ids(plan)\n",
    "    print(f\"üîç Extracted GSE IDs: {gse_ids}\")\n",
    "    return {**state, \"selected_gses\": gse_ids}\n",
    "\n",
    "# DEG Agent with Enhanced LLM Classification from GEO Series Matrix\n",
    "\n",
    "def extract_json_block(text):\n",
    "    match = re.search(r\"```json\\s*({.*?})\\s*```\", text, re.DOTALL)\n",
    "    if match:\n",
    "        return json.loads(match.group(1))\n",
    "    return json.loads(text)\n",
    "\n",
    "def decompress_all_in_dir(root_dir: str):\n",
    "    \"\"\"\n",
    "    Decompress .gz and .tar files only if no uncompressed files exist in each GSE subdirectory.\n",
    "    \"\"\"\n",
    "    print(f\"üîç Scanning for compressed files under: {root_dir}\")\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        # Skip decompression if the folder already has non-compressed data files\n",
    "        if any(f.lower().endswith(('.csv', '.tsv', '.txt', '.xlsx')) for f in filenames):\n",
    "            continue\n",
    "\n",
    "        for fname in filenames:\n",
    "            full_path = os.path.join(dirpath, fname)\n",
    "            if tarfile.is_tarfile(full_path):\n",
    "                try:\n",
    "                    print(f\"üì¶ Extracting TAR: {full_path}\")\n",
    "                    with tarfile.open(full_path, \"r:*\") as tar:\n",
    "                        tar.extractall(path=dirpath)\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Failed to extract TAR {fname}: {e}\")\n",
    "            elif fname.endswith(\".gz\") and not fname.endswith(\".tar.gz\"):\n",
    "                out_path = os.path.join(dirpath, fname[:-3])\n",
    "                if os.path.exists(out_path):\n",
    "                    print(f\"‚ö†Ô∏è Skipping (already exists): {out_path}\")\n",
    "                    continue\n",
    "                try:\n",
    "                    print(f\"üóúÔ∏è Decompressing GZ: {full_path} ‚Üí {out_path}\")\n",
    "                    with gzip.open(full_path, 'rb') as f_in:\n",
    "                        with open(out_path, 'wb') as f_out:\n",
    "                            shutil.copyfileobj(f_in, f_out)\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Failed to decompress GZ {fname}: {e}\")\n",
    "def decompress_gse_in_dir(gse_id: str, root_dir: str = \"rna_seq_analysis\"):\n",
    "    \"\"\"\n",
    "    Decompress .gz and .tar files only if no uncompressed files exist\n",
    "    in the subdirectory for the specific GSE.\n",
    "    \"\"\"\n",
    "    gse_path = os.path.join(root_dir, gse_id)\n",
    "    print(f\"üîç Scanning for compressed files in: {gse_path}\")\n",
    "    if not os.path.exists(gse_path):\n",
    "        print(f\"‚ö†Ô∏è Directory does not exist: {gse_path}\")\n",
    "        return\n",
    "\n",
    "    filenames = os.listdir(gse_path)\n",
    "    if any(f.lower().endswith(('.csv', '.tsv', '.txt', '.xlsx')) for f in filenames):\n",
    "        return\n",
    "\n",
    "    for fname in filenames:\n",
    "        full_path = os.path.join(gse_path, fname)\n",
    "        if tarfile.is_tarfile(full_path):\n",
    "            try:\n",
    "                print(f\"üì¶ Extracting TAR: {full_path}\")\n",
    "                with tarfile.open(full_path, \"r:*\") as tar:\n",
    "                    tar.extractall(path=gse_path)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to extract TAR {fname}: {e}\")\n",
    "        elif fname.endswith(\".gz\") and not fname.endswith(\".tar.gz\"):\n",
    "            out_path = os.path.join(gse_path, fname[:-3])\n",
    "            if os.path.exists(out_path):\n",
    "                print(f\"‚ö†Ô∏è Skipping (already exists): {out_path}\")\n",
    "                continue\n",
    "            try:\n",
    "                print(f\"üóúÔ∏è Decompressing GZ: {full_path} ‚Üí {out_path}\")\n",
    "                with gzip.open(full_path, 'rb') as f_in:\n",
    "                    with open(out_path, 'wb') as f_out:\n",
    "                        shutil.copyfileobj(f_in, f_out)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to decompress GZ {fname}: {e}\")\n",
    "\n",
    "def get_series_matrix_url(gse_id):\n",
    "    prefix = gse_id[:6]\n",
    "    return f\"https://ftp.ncbi.nlm.nih.gov/geo/series/{prefix}nnn/{gse_id}/matrix/{gse_id}_series_matrix.txt.gz\"\n",
    "\n",
    "def download_series_matrix(gse_id, output_dir=\"geofetch_metadata\"):\n",
    "    url = get_series_matrix_url(gse_id)\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_path = output_dir / f\"{gse_id}_series_matrix.txt.gz\"\n",
    "    if output_path.exists():\n",
    "        return str(output_path)\n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(output_path, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        return str(output_path)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Could not download series matrix for {gse_id}\")\n",
    "\n",
    "def infer_conditions_with_labels_from_series_matrix(series_matrix_path, gse_id, max_samples=30):\n",
    "    characteristics_matrix, sample_accessions = [], []\n",
    "    with gzip.open(series_matrix_path, \"rt\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"!Sample_characteristics_ch1\"):\n",
    "                values = line.strip().split(\"\\t\")[1:]\n",
    "                characteristics_matrix.append(values)\n",
    "            elif line.startswith(\"!Sample_geo_accession\"):\n",
    "                sample_accessions = line.strip().split(\"\\t\")[1:]\n",
    "    if not characteristics_matrix or not sample_accessions:\n",
    "        raise ValueError(\"Missing sample characteristics or accessions.\")\n",
    "    characteristics_matrix = list(zip(*characteristics_matrix))\n",
    "    sample_data = [\n",
    "        {\"GSM\": gsm.strip('\"'), \"characteristics\": [c.strip('\"') for c in char_fields if c.strip()]}\n",
    "        for gsm, char_fields in zip(sample_accessions, characteristics_matrix)\n",
    "    ]\n",
    "    metadata_json = json.dumps(sample_data[:max_samples], indent=2)\n",
    "    prompt = f\"\"\"\n",
    "You are a biomedical data curator.\n",
    "\n",
    "Below is a list of samples from GEO study **{gse_id}**. Each sample includes multiple characteristics such as diagnosis, sex, age, and treatment.\n",
    "\n",
    "Your task:\n",
    "1. Classify each sample as either part of the **control** group or the **treated/disease** group.\n",
    "2. Extract a **short biological label** for each sample from its characteristics (e.g., \\\"ALS\\\", \\\"healthy\\\", \\\"treated with ..\\\").\n",
    "\n",
    "Here is the list:\n",
    "```json\n",
    "{metadata_json}\n",
    "```\n",
    "Return only a **valid JSON object** like:\n",
    "```json\n",
    "{{\n",
    "  \"GSM8077414\": {{ \"group\": \"control\", \"label\": \"healthy\" }},\n",
    "  \"GSM8077415\": {{ \"group\": \"treated\", \"label\": \"ALS\" }}\n",
    "}}\n",
    "---\n",
    "\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        temperature=0.2,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    content = response.choices[0].message.content.strip()\n",
    "    mapping = extract_json_block(content)\n",
    "    if not isinstance(mapping, dict) or not all(isinstance(v, dict) and \"group\" in v and \"label\" in v for v in mapping.values()):\n",
    "        raise ValueError(\"Invalid response from LLM\")\n",
    "    return mapping\n",
    "\n",
    "def get_group_label_mapping_from_gse(gse_id, output_dir=\"geofetch_metadata\", max_samples=30):\n",
    "    path = download_series_matrix(gse_id, output_dir=output_dir)\n",
    "    return infer_conditions_with_labels_from_series_matrix(path, gse_id, max_samples=max_samples)\n",
    "\n",
    "def download_and_extract_gse(gse_id, base_dir=\"rna_seq_analysis\"):\n",
    "    \"\"\"\n",
    "    Downloads and extracts supplementary files for a given GSE.\n",
    "    \"\"\"\n",
    "    gse_dir = Path(base_dir) / gse_id\n",
    "    gse_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    files = scrape_geo_supplementary_downloads(gse_id)\n",
    "    if not files:\n",
    "        print(f\"‚ö†Ô∏è No supplementary files found for {gse_id}\")\n",
    "        return str(gse_dir)\n",
    "\n",
    "    file_name, url = files[0]\n",
    "    if not file_name:\n",
    "        parsed = urlparse(url)\n",
    "        qs = parse_qs(parsed.query)\n",
    "        file_name = unquote(qs.get(\"file\", [\"unnamed_file\"])[0])\n",
    "\n",
    "    file_path = gse_dir / file_name\n",
    "    print(f\"üì• Downloading {file_name} ‚Üí {file_path.as_posix()}\")\n",
    "\n",
    "    try:\n",
    "        subprocess.run([\"curl\", \"-L\", \"-o\", file_path.as_posix(), url], check=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ùå Download failed for {gse_id}: {e}\")\n",
    "        return str(gse_dir)\n",
    "\n",
    "    try:\n",
    "        if tarfile.is_tarfile(file_path):\n",
    "            print(\"üóÇÔ∏è Extracting TAR...\")\n",
    "            with tarfile.open(file_path, \"r:*\") as tar:\n",
    "                tar.extractall(path=gse_dir)\n",
    "        elif file_path.suffix == \".gz\" and not file_path.name.endswith(\".tar.gz\"):\n",
    "            unzipped_path = file_path.with_suffix(\"\")\n",
    "            print(\"üóÇÔ∏è Extracting GZ...\")\n",
    "            with gzip.open(file_path, 'rb') as f_in:\n",
    "                with open(unzipped_path, 'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Extraction failed for {gse_id}: {e}\")\n",
    "def enrichr_api(input_data: List[str]) -> dict:\n",
    "    \"\"\"\n",
    "    Perform enrichment analysis using the Enrichr API.\n",
    "  \n",
    "    Args:\n",
    "        input_data (List[str]): Input data with a list of gene symbols.\n",
    "\n",
    "    Returns:\n",
    "        dict: Enrichment results or error details.\n",
    "    \"\"\"\n",
    "    gene_list = input_data\n",
    "    \n",
    "    try:\n",
    "        # Perform enrichment analysis\n",
    "        enr = gp.enrichr(\n",
    "            gene_list=gene_list, \n",
    "            gene_sets='KEGG_2016', \n",
    "            outdir='test/enrichr_kegg_tumor'\n",
    "        )\n",
    "        \n",
    "        # Extract top results as JSON\n",
    "        enrichment_results = enr.res2d.head().to_dict(orient='records')\n",
    "\n",
    "        return {\n",
    "            \"enrichment_results\": enrichment_results\n",
    "        }\n",
    "    except ValidationError as e:\n",
    "        return {\n",
    "            \"error\": f\"Validation error: {e.errors()}\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "    return str(gse_dir)\n",
    "def run_deg_for_multiple_gses_debug(state):\n",
    "    gse_ids = state.get(\"selected_gses\", [])\n",
    "    if not gse_ids:\n",
    "        print(\"No selected GSEs found.\")\n",
    "        return state\n",
    "    deg_paths = {}\n",
    "\n",
    "    # Pre-decompress all GSE folders\n",
    "    decompress_all_in_dir(\"rna_seq_analysis\")\n",
    "\n",
    "    for gse_id in gse_ids:\n",
    "        try:\n",
    "            print(f\"Running DESeq2 for: {gse_id}\")\n",
    "            download_dir = download_and_extract_gse(gse_id)\n",
    "            counts_df = find_and_merge_count_files(gse_id)\n",
    "            if counts_df is None or counts_df.empty:\n",
    "                continue\n",
    "            group_label_mapping = get_group_label_mapping_from_gse(gse_id)\n",
    "            metadata_df = pd.DataFrame.from_dict(group_label_mapping, orient=\"index\")\n",
    "            metadata_df.index.name = \"sample_geo_accession\"\n",
    "            metadata_df = metadata_df[[\"group\"]].rename(columns={\"group\": \"condition\"})\n",
    "            print(metadata_df)\n",
    "            counts_df.columns = [gsm if gsm in metadata_df.index else gsm for gsm in counts_df.columns]\n",
    "            valid_samples = metadata_df.index.intersection(counts_df.columns)\n",
    "            counts_df = counts_df[valid_samples]\n",
    "            metadata_df = metadata_df.loc[valid_samples]\n",
    "            if counts_df.empty or metadata_df.empty:\n",
    "                continue\n",
    "            dds = DeseqDataSet(\n",
    "                counts=counts_df.T,\n",
    "                metadata=metadata_df,\n",
    "                design_factors=\"condition\",\n",
    "                refit_cooks=True,\n",
    "            )\n",
    "            dds.deseq2()\n",
    "            ds = DeseqStats(dds, contrast=[\"condition\", \"treated\", \"control\"])\n",
    "            ds.summary()\n",
    "            output_csv = f\"rna_seq_analysis/{gse_id}_deseq2_results.csv\"\n",
    "            ds.results_df.to_csv(output_csv, index=False)\n",
    "            deg_paths[gse_id] = output_csv\n",
    "        except Exception as e:\n",
    "            print(f\"Error running DESeq2 for {gse_id}: {e}\")\n",
    "    return {**state, \"deg_results\": deg_paths}\n",
    "\n",
    "def run_deg_for_single_gse_debug(state):\n",
    "    gse_id = state.get(\"selected_gse\")\n",
    "    if not gse_id:\n",
    "        print(\"No selected GSE found.\")\n",
    "        return state\n",
    "\n",
    "    deg_paths = {}\n",
    "    decompress_all_in_dir(\"rna_seq_analysis\")\n",
    "\n",
    "    try:\n",
    "        print(f\"Running DESeq2 for: {gse_id}\")\n",
    "        download_dir = download_and_extract_gse(gse_id)\n",
    "        counts_df = find_and_merge_count_files(gse_id)\n",
    "        if counts_df is None or counts_df.empty:\n",
    "            return state\n",
    "\n",
    "        group_label_mapping = get_group_label_mapping_from_gse(gse_id)\n",
    "        metadata_df = pd.DataFrame.from_dict(group_label_mapping, orient=\"index\")\n",
    "        metadata_df.index.name = \"sample_geo_accession\"\n",
    "        metadata_df = metadata_df[[\"group\"]].rename(columns={\"group\": \"condition\"})\n",
    "        print(metadata_df)\n",
    "\n",
    "        counts_df.columns = [gsm if gsm in metadata_df.index else gsm for gsm in counts_df.columns]\n",
    "        valid_samples = metadata_df.index.intersection(counts_df.columns)\n",
    "        counts_df = counts_df[valid_samples]\n",
    "        metadata_df = metadata_df.loc[valid_samples]\n",
    "\n",
    "        if counts_df.empty or metadata_df.empty:\n",
    "            return state\n",
    "\n",
    "        dds = DeseqDataSet(\n",
    "            counts=counts_df.T,\n",
    "            metadata=metadata_df,\n",
    "            design_factors=\"condition\",\n",
    "            refit_cooks=True,\n",
    "        )\n",
    "        dds.deseq2()\n",
    "        ds = DeseqStats(dds, contrast=[\"condition\", \"treated\", \"control\"])\n",
    "        ds.summary()\n",
    "\n",
    "        output_csv = f\"rna_seq_analysis/{gse_id}_deseq2_results.csv\"\n",
    "        ds.results_df.to_csv(output_csv, index=False)\n",
    "        deg_paths[gse_id] = output_csv\n",
    "    except Exception as e:\n",
    "        print(f\"Error running DESeq2 for {gse_id}: {e}\")\n",
    "\n",
    "    return {**state, \"deg_results\": deg_paths}\n",
    "def run_deg_for_single_gse_debug(state):\n",
    "    gse_id = state.get(\"selected_gse\")\n",
    "    if not gse_id:\n",
    "        print(\"No selected GSE found.\")\n",
    "        return state\n",
    "\n",
    "    deg_paths = {}\n",
    "    decompress_gse_in_dir(gse_id)\n",
    "\n",
    "    try:\n",
    "        print(f\"Running DESeq2 for: {gse_id}\")\n",
    "        download_dir = download_and_extract_gse(gse_id)\n",
    "        counts_df = find_and_merge_count_files(gse_id)\n",
    "        if counts_df is None or counts_df.empty:\n",
    "            return state\n",
    "\n",
    "        group_label_mapping = get_group_label_mapping_from_gse(gse_id)\n",
    "        metadata_df = pd.DataFrame.from_dict(group_label_mapping, orient=\"index\")\n",
    "        metadata_df.index.name = \"sample_geo_accession\"\n",
    "        metadata_df = metadata_df[[\"group\"]].rename(columns={\"group\": \"condition\"})\n",
    "        print(metadata_df)\n",
    "\n",
    "        counts_df.columns = [gsm if gsm in metadata_df.index else gsm for gsm in counts_df.columns]\n",
    "        valid_samples = metadata_df.index.intersection(counts_df.columns)\n",
    "        counts_df = counts_df[valid_samples]\n",
    "        metadata_df = metadata_df.loc[valid_samples]\n",
    "\n",
    "        if counts_df.empty or metadata_df.empty:\n",
    "            return state\n",
    "\n",
    "        dds = DeseqDataSet(\n",
    "            counts=counts_df.T,\n",
    "            metadata=metadata_df,\n",
    "            design_factors=\"condition\",\n",
    "            refit_cooks=True,\n",
    "        )\n",
    "        dds.deseq2()\n",
    "        ds = DeseqStats(dds, contrast=[\"condition\", \"treated\", \"control\"])\n",
    "        ds.summary()\n",
    "\n",
    "        output_csv = f\"rna_seq_analysis/{gse_id}_deseq2_results.csv\"\n",
    "        ds.results_df.to_csv(output_csv, index=False)\n",
    "        deg_paths[gse_id] = output_csv\n",
    "    except Exception as e:\n",
    "        print(f\"Error running DESeq2 for {gse_id}: {e}\")\n",
    "\n",
    "    return {**state, \"deg_results\": deg_paths}\n",
    "def find_and_merge_count_files(gse_id: str, base_dir=\"rna_seq_analysis\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Finds and merges all single-sample count files (including .gz) into a gene x sample matrix.\n",
    "    Ensures no duplicate sample names are merged.\n",
    "    \"\"\"\n",
    "    gse_dir = Path(base_dir) / gse_id\n",
    "    if not gse_dir.exists():\n",
    "        print(f\"‚ùå GSE folder not found: {gse_dir}\")\n",
    "        return None\n",
    "\n",
    "    patterns = [\"*.csv\", \"*.tsv\", \"*.txt\", \"*.xlsx\", \"*.gz\"]\n",
    "    files = []\n",
    "    seen_uncompressed = set()\n",
    "    for pattern in patterns:\n",
    "        for f in gse_dir.glob(pattern):\n",
    "            if f.suffix == \".gz\":\n",
    "                uncompressed_name = f.with_suffix(\"\")\n",
    "                if uncompressed_name.name in seen_uncompressed:\n",
    "                    print(f\"‚ö†Ô∏è Skipping compressed duplicate: {f.name}\")\n",
    "                    continue\n",
    "            else:\n",
    "                seen_uncompressed.add(f.name)\n",
    "            files.append(f)\n",
    "\n",
    "    if not files:\n",
    "        print(f\"‚ö†Ô∏è No count files found in {gse_dir}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"üîç Found {len(files)} count files for {gse_id}\")\n",
    "    merged_df = None\n",
    "    used_column_names = set()\n",
    "\n",
    "    for file in files:\n",
    "        try:\n",
    "            sample_base = file.stem.split(\"_\")[0] if \"_\" in file.stem else file.stem\n",
    "            if file.suffix == \".gz\":\n",
    "                with gzip.open(file, 'rt') as f:\n",
    "                    df = pd.read_csv(f, sep=\"\t\", index_col=0)\n",
    "            elif file.suffix == \".csv\":\n",
    "                df = pd.read_csv(file, index_col=0)\n",
    "            elif file.suffix == \".xlsx\":\n",
    "                df = pd.read_excel(file, index_col=0)\n",
    "            else:\n",
    "                df = pd.read_csv(file, sep=\"\t\", index_col=0)\n",
    "\n",
    "            df = df.apply(pd.to_numeric, errors=\"coerce\").fillna(0)\n",
    "\n",
    "            if df.shape[1] == 1:\n",
    "                col_name = sample_base\n",
    "            elif sample_base in df.columns:\n",
    "                df = df[[sample_base]]\n",
    "                col_name = sample_base\n",
    "            else:\n",
    "                df = df.iloc[:, [0]]\n",
    "                col_name = sample_base\n",
    "\n",
    "            original_col_name = col_name\n",
    "            counter = 1\n",
    "            while col_name in used_column_names:\n",
    "                col_name = f\"{original_col_name}_{counter}\"\n",
    "                counter += 1\n",
    "            used_column_names.add(col_name)\n",
    "\n",
    "            df.columns = [col_name]\n",
    "\n",
    "            if merged_df is None:\n",
    "                merged_df = df\n",
    "            else:\n",
    "                merged_df = merged_df.join(df, how=\"outer\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to read {file.name}: {e}\")\n",
    "\n",
    "    if merged_df is not None:\n",
    "        merged_df = merged_df.fillna(0).astype(int) + 1\n",
    "        print(f\"‚úÖ Final merged count matrix shape: {merged_df.shape}\")\n",
    "        return merged_df\n",
    "    else:\n",
    "        print(\"‚ùå No valid count data could be merged.\")\n",
    "        return None\n",
    "\n",
    "def run_deg_for_single_gse_debug(state):\n",
    "    gse_id = state.get(\"selected_gse\")\n",
    "    if not gse_id:\n",
    "        print(\"No selected GSE found.\")\n",
    "        return state\n",
    "\n",
    "    deg_paths = {}\n",
    "    decompress_gse_in_dir(gse_id)\n",
    "\n",
    "    try:\n",
    "        print(f\"Running DESeq2 for: {gse_id}\")\n",
    "        download_dir = download_and_extract_gse(gse_id)\n",
    "        counts_df = find_and_merge_count_files(gse_id)\n",
    "        if counts_df is None or counts_df.empty:\n",
    "            return state\n",
    "\n",
    "        group_label_mapping = get_group_label_mapping_from_gse(gse_id)\n",
    "        metadata_df = pd.DataFrame.from_dict(group_label_mapping, orient=\"index\")\n",
    "        metadata_df.index.name = \"sample_geo_accession\"\n",
    "        metadata_df = metadata_df[[\"group\"]].rename(columns={\"group\": \"condition\"})\n",
    "        print(metadata_df)\n",
    "\n",
    "        counts_df.columns = [gsm if gsm in metadata_df.index else gsm for gsm in counts_df.columns]\n",
    "        valid_samples = metadata_df.index.intersection(counts_df.columns)\n",
    "        counts_df = counts_df[valid_samples]\n",
    "        metadata_df = metadata_df.loc[valid_samples]\n",
    "\n",
    "        if counts_df.empty or metadata_df.empty:\n",
    "            return state\n",
    "\n",
    "        dds = DeseqDataSet(\n",
    "            counts=counts_df.T,\n",
    "            metadata=metadata_df,\n",
    "            design_factors=\"condition\",\n",
    "            refit_cooks=True,\n",
    "        )\n",
    "        dds.deseq2()\n",
    "        ds = DeseqStats(dds, contrast=[\"condition\", \"treated\", \"control\"])\n",
    "        ds.summary()\n",
    "\n",
    "        output_csv = f\"rna_seq_analysis/{gse_id}_deseq2_results.csv\"\n",
    "        ds.results_df.to_csv(output_csv, index=False)\n",
    "        deg_paths[gse_id] = output_csv\n",
    "\n",
    "        # Filter up- and down-regulated gene symbols\n",
    "        deg_df = ds.results_df\n",
    "        up_genes = deg_df[(deg_df['log2FoldChange'] > 0.5) & (deg_df['padj'] < 0.05)].index.tolist()\n",
    "        down_genes = deg_df[(deg_df['log2FoldChange'] < -0.5) & (deg_df['padj'] < 0.05)].index.tolist()\n",
    "        up_genes = [up_genes.upper() for gene in up_genes]\n",
    "        down_genes = [down_genes.upper() for gene in down_genes]\n",
    "        print(\"\\nüß¨ Pathway Enrichment - Upregulated Genes:\")\n",
    "        print(up_genes)\n",
    "        print(enrichr_api(up_genes))\n",
    "\n",
    "        print(\"\\nüß¨ Pathway Enrichment - Downregulated Genes:\")\n",
    "        print(down_genes)\n",
    "        print(enrichr_api(down_genes))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error running DESeq2 for {gse_id}: {e}\")\n",
    "\n",
    "    return {**state, \"deg_results\": deg_paths}\n",
    "############# test agent 1 and 2 connection\n",
    "# Build the pipeline\n",
    "workflow = StateGraph(state_schema=AgentState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"Ingestor\", RunnableLambda(ingest_and_prepare))\n",
    "workflow.add_node(\"Analyst\", RunnableLambda(analyze_metadata_and_plan))\n",
    "workflow.add_node(\"GSEListExtractor\", RunnableLambda(extract_gse_list_from_plan))\n",
    "workflow.add_node(\"DEGRunner\", RunnableLambda(run_deg_for_multiple_gses_debug))\n",
    "# Connect nodes\n",
    "\n",
    "workflow.set_entry_point(\"Ingestor\")\n",
    "workflow.add_edge(\"Ingestor\", \"Analyst\")\n",
    "workflow.add_edge(\"Analyst\", \"GSEListExtractor\")\n",
    "workflow.add_edge(\"GSEListExtractor\", \"DEGRunner\")\n",
    "\n",
    "# Final output node\n",
    "workflow.set_finish_point(\"DEGRunner\")\n",
    "#workflow.set_finish_point(\"GSEListExtractor\")\n",
    "# Compile the graph\n",
    "graph = workflow.compile()\n",
    "from IPython.display import Image, display\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4604cc1a-5646-4e19-a94b-19cbffd20072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Scanning for compressed files in: rna_seq_analysis\\GSE242272\n",
      "Running DESeq2 for: GSE242272\n",
      "üîç Scraping GEO page: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE242272\n",
      "‚úÖ Page fetched successfully\n",
      "üîé Found 23 total tables\n",
      "üìã Found supplementary file table at index 0\n",
      "üìÑ Table has 75 data rows\n",
      "üì• Downloading GSE242272_RAW.tar ‚Üí rna_seq_analysis/GSE242272/GSE242272_RAW.tar\n",
      "üóÇÔ∏è Extracting TAR...\n",
      "‚ö†Ô∏è Skipping compressed duplicate: GSM7757588_CD8_24h_Vehicle1.tabular.txt.gz\n",
      "‚ö†Ô∏è Skipping compressed duplicate: GSM7757589_CD8_24h_Vehicle2.tabular.txt.gz\n",
      "‚ö†Ô∏è Skipping compressed duplicate: GSM7757590_CD8_24h_Vehicle3.tabular.txt.gz\n",
      "‚ö†Ô∏è Skipping compressed duplicate: GSM7757591_CD8_24h_PGE2_1.tabular.txt.gz\n",
      "‚ö†Ô∏è Skipping compressed duplicate: GSM7757592_CD8_24h_PGE2_2.tabular.txt.gz\n",
      "‚ö†Ô∏è Skipping compressed duplicate: GSM7757593_CD8_24h_PGE2_3.tabular.txt.gz\n",
      "‚ö†Ô∏è Skipping compressed duplicate: GSM7757594_CD8_48h_Vehicle1.tabular.txt.gz\n",
      "‚ö†Ô∏è Skipping compressed duplicate: GSM7757595_CD8_48h_Vehicle2.tabular.txt.gz\n",
      "‚ö†Ô∏è Skipping compressed duplicate: GSM7757596_CD8_48h_Vehicle3.tabular.txt.gz\n",
      "‚ö†Ô∏è Skipping compressed duplicate: GSM7757597_CD8_48h_PGE2_1.tabular.txt.gz\n",
      "‚ö†Ô∏è Skipping compressed duplicate: GSM7757598_CD8_48h_PGE2_2.tabular.txt.gz\n",
      "‚ö†Ô∏è Skipping compressed duplicate: GSM7757599_CD8_48h_PGE2_3.tabular.txt.gz\n",
      "‚ö†Ô∏è Skipping compressed duplicate: GSM7757600_CD8_60h_Vehicle1.tabular.txt.gz\n",
      "‚ö†Ô∏è Skipping compressed duplicate: GSM7757601_CD8_60h_Vehicle2.tabular.txt.gz\n",
      "‚ö†Ô∏è Skipping compressed duplicate: GSM7757602_CD8_60h_Vehicle3.tabular.txt.gz\n",
      "‚ö†Ô∏è Skipping compressed duplicate: GSM7757603_CD8_60h_PGE2_1.tabular.txt.gz\n",
      "‚ö†Ô∏è Skipping compressed duplicate: GSM7757604_CD8_60h_PGE2_2.tabular.txt.gz\n",
      "‚ö†Ô∏è Skipping compressed duplicate: GSM7757605_CD8_60h_PGE2_3.tabular.txt.gz\n",
      "üîç Found 18 count files for GSE242272\n",
      "‚úÖ Final merged count matrix shape: (25238, 18)\n",
      "                     condition\n",
      "sample_geo_accession          \n",
      "GSM7757588             control\n",
      "GSM7757589             control\n",
      "GSM7757590             control\n",
      "GSM7757591             treated\n",
      "GSM7757592             treated\n",
      "GSM7757593             treated\n",
      "GSM7757594             control\n",
      "GSM7757595             control\n",
      "GSM7757596             control\n",
      "GSM7757597             treated\n",
      "GSM7757598             treated\n",
      "GSM7757599             treated\n",
      "GSM7757600             control\n",
      "GSM7757601             control\n",
      "GSM7757602             control\n",
      "GSM7757603             treated\n",
      "GSM7757604             treated\n",
      "GSM7757605             treated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\difen\\AppData\\Local\\Temp\\ipykernel_52140\\3263277725.py:991: DeprecationWarning: design_factors is deprecated and will soon be removed.Please consider providing a formulaic formula using the design argumentinstead.\n",
      "  dds = DeseqDataSet(\n",
      "Fitting size factors...\n",
      "... done in 0.02 seconds.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using None as control genes, passed at DeseqDataSet initialization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting dispersions...\n",
      "... done in 1.67 seconds.\n",
      "\n",
      "Fitting dispersion trend curve...\n",
      "C:\\Users\\difen\\venv1\\Lib\\site-packages\\pydeseq2\\dds.py:805: UserWarning: The dispersion trend curve fitting did not converge. Switching to a mean-based dispersion trend.\n",
      "  self._fit_parametric_dispersion_trend(vst)\n",
      "... done in 0.36 seconds.\n",
      "\n",
      "Fitting MAP dispersions...\n",
      "... done in 2.83 seconds.\n",
      "\n",
      "Fitting LFCs...\n",
      "... done in 1.47 seconds.\n",
      "\n",
      "Calculating cook's distance...\n",
      "... done in 0.03 seconds.\n",
      "\n",
      "Replacing 8 outlier genes.\n",
      "\n",
      "Fitting dispersions...\n",
      "... done in 0.01 seconds.\n",
      "\n",
      "Fitting MAP dispersions...\n",
      "... done in 0.01 seconds.\n",
      "\n",
      "Fitting LFCs...\n",
      "... done in 0.01 seconds.\n",
      "\n",
      "Running Wald tests...\n",
      "... done in 0.95 seconds.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log2 fold change & Wald test p-value: condition treated vs control\n",
      "                  baseMean  log2FoldChange     lfcSE          stat    pvalue  \\\n",
      "0610005C13Rik                                                                  \n",
      "0610009B22Rik   165.222222   -1.709557e-01  0.255686 -6.686154e-01  0.503741   \n",
      "0610009E02Rik    18.055556   -2.663430e-02  0.899612 -2.960642e-02  0.976381   \n",
      "0610009L18Rik     9.944444    1.128941e-01  0.280040  4.031350e-01  0.686849   \n",
      "0610010F05Rik   290.888889   -3.779754e-01  0.124858 -3.027232e+00  0.002468   \n",
      "0610010K14Rik   703.111111   -2.735753e-03  0.185979 -1.470998e-02  0.988264   \n",
      "...                    ...             ...       ...           ...       ...   \n",
      "Zyx            2826.500000    7.072633e-01  0.304142  2.325437e+00  0.020049   \n",
      "Zzef1          1112.388889    2.637222e-01  0.611036  4.315986e-01  0.666033   \n",
      "Zzz3           1074.277778   -9.867012e-02  0.211156 -4.672854e-01  0.640296   \n",
      "a                12.500000   -5.052348e-01  0.659594 -7.659784e-01  0.443689   \n",
      "ccdc198           1.000000    5.451914e-17  0.710913  7.668891e-17  1.000000   \n",
      "\n",
      "                   padj  \n",
      "0610005C13Rik            \n",
      "0610009B22Rik  0.994986  \n",
      "0610009E02Rik  1.000000  \n",
      "0610009L18Rik  1.000000  \n",
      "0610010F05Rik  0.062087  \n",
      "0610010K14Rik  1.000000  \n",
      "...                 ...  \n",
      "Zyx            0.247045  \n",
      "Zzef1          1.000000  \n",
      "Zzz3           1.000000  \n",
      "a              0.981034  \n",
      "ccdc198             NaN  \n",
      "\n",
      "[25238 rows x 6 columns]\n",
      "Error running DESeq2 for GSE242272: 'list' object has no attribute 'upper'\n",
      "\n",
      "‚úÖ DEG Results by GSE ID:\n",
      "- GSE242272: rna_seq_analysis/GSE242272_deseq2_results.csv\n"
     ]
    }
   ],
   "source": [
    "initial_state: Dict = {\n",
    "    \"query_gene\": \"\",\n",
    "    \"pathway_info\": {\"genes\": [], \"drugs\": [], \"pathways\": []},\n",
    "    \"metadata\": {},\n",
    "    \"gse_list\": [],\n",
    "    \"research_plan\": \"\",\n",
    "    \"selected_gse\": \"GSE242272\",\n",
    "    \"selected_gses\": [\"GSE242272\"],\n",
    "    \"deg_results\": {}\n",
    "}\n",
    "# Run the DEG analysis directly\n",
    "result_state = run_deg_for_single_gse_debug(initial_state)\n",
    "\n",
    "# Output the DEG results\n",
    "print(\"\\n‚úÖ DEG Results by GSE ID:\")\n",
    "for gse_id, path in result_state.get(\"deg_results\", {}).items():\n",
    "    print(f\"- {gse_id}: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9fb9ee1-168a-49f7-808c-58eafbc53167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test enrichr_api function independently with a known gene list\n",
    "from gseapy import enrichr\n",
    "\n",
    "test_genes = [\"TP53\", \"EGFR\", \"MYC\", \"BRCA1\", \"CDK2\"]\n",
    "\n",
    "# Run the Enrichr API\n",
    "try:\n",
    "    result = enrichr_api(test_genes)\n",
    "    result\n",
    "except Exception as e:\n",
    "    {\"error\": str(e)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9d1fcc2-bd9b-4d07-9218-0c7cbdcbccf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query_gene': 'MAP4K1',\n",
       " 'pathway_info': {'genes': [], 'drugs': [], 'pathways': []},\n",
       " 'metadata': {},\n",
       " 'gse_list': [],\n",
       " 'research_plan': 'Study GSE242272 includes drug-treated and control samples suitable for DEG analysis.',\n",
       " 'selected_gse': 'GSE242272',\n",
       " 'selected_gses': ['GSE242272'],\n",
       " 'deg_results': {}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e440fd1-009b-468f-b1ea-396306aa9f07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Gene_set': 'KEGG_2016',\n",
       "  'Term': 'PI3K-Akt signaling pathway Homo sapiens hsa04151',\n",
       "  'Overlap': '5/341',\n",
       "  'P-value': 1.39946070264984e-09,\n",
       "  'Adjusted P-value': 8.956548496958976e-08,\n",
       "  'Old P-value': 0,\n",
       "  'Old Adjusted P-value': 0,\n",
       "  'Odds Ratio': 98295.0,\n",
       "  'Combined Score': 2003957.748690934,\n",
       "  'Genes': 'MYC;CDK2;BRCA1;TP53;EGFR'},\n",
       " {'Gene_set': 'KEGG_2016',\n",
       "  'Term': 'Bladder cancer Homo sapiens hsa05219',\n",
       "  'Overlap': '3/41',\n",
       "  'P-value': 7.972406919171286e-08,\n",
       "  'Adjusted P-value': 2.551170214134812e-06,\n",
       "  'Old P-value': 0,\n",
       "  'Old Adjusted P-value': 0,\n",
       "  'Odds Ratio': 787.7763157894736,\n",
       "  'Combined Score': 12875.963057851615,\n",
       "  'Genes': 'MYC;TP53;EGFR'},\n",
       " {'Gene_set': 'KEGG_2016',\n",
       "  'Term': 'Endometrial cancer Homo sapiens hsa05213',\n",
       "  'Overlap': '3/52',\n",
       "  'P-value': 1.6514690324614622e-07,\n",
       "  'Adjusted P-value': 3.5231339359177858e-06,\n",
       "  'Old P-value': 0,\n",
       "  'Old Adjusted P-value': 0,\n",
       "  'Odds Ratio': 610.5918367346939,\n",
       "  'Combined Score': 9535.26494342797,\n",
       "  'Genes': 'MYC;TP53;EGFR'},\n",
       " {'Gene_set': 'KEGG_2016',\n",
       "  'Term': 'MicroRNAs in cancer Homo sapiens hsa05206',\n",
       "  'Overlap': '4/297',\n",
       "  'P-value': 2.3552033963808225e-07,\n",
       "  'Adjusted P-value': 3.768325434209316e-06,\n",
       "  'Old P-value': 0,\n",
       "  'Old Adjusted P-value': 0,\n",
       "  'Odds Ratio': 268.96928327645054,\n",
       "  'Combined Score': 4104.866260148936,\n",
       "  'Genes': 'MYC;BRCA1;TP53;EGFR'},\n",
       " {'Gene_set': 'KEGG_2016',\n",
       "  'Term': 'Central carbon metabolism in cancer Homo sapiens hsa05230',\n",
       "  'Overlap': '3/67',\n",
       "  'P-value': 3.575808324848017e-07,\n",
       "  'Adjusted P-value': 4.577034655805462e-06,\n",
       "  'Old P-value': 0,\n",
       "  'Old Adjusted P-value': 0,\n",
       "  'Odds Ratio': 467.1328125,\n",
       "  'Combined Score': 6934.074808759252,\n",
       "  'Genes': 'MYC;TP53;EGFR'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gseapy as gp\n",
    "input_data = [gene.upper() for gene in test_genes]\n",
    "enr = gp.enrichr(\n",
    "            gene_list=input_data,\n",
    "            gene_sets='KEGG_2016',\n",
    "            outdir='test/enrichr_kegg_tumor'\n",
    "        )\n",
    "enrichment_results = enr.res2d.head().to_dict(orient='records')\n",
    "enrichment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bfd36a1f-19f8-4733-a147-319a56a4bd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrichr_api(input_data: List[str]) -> dict:\n",
    "    input_data = [gene.upper() for gene in input_data]  # Ensure all gene symbols are uppercase\n",
    "    print(input_data)\n",
    "    try:\n",
    "        enr = gp.enrichr(\n",
    "            gene_list=input_data,\n",
    "            gene_sets='KEGG_2016',\n",
    "            outdir='test/enrichr_kegg_tumor'\n",
    "        )\n",
    "        enrichment_results = enr.res2d.head().to_dict(orient='records')\n",
    "        return {\"enrichment_results\": enrichment_results}\n",
    "    except ValidationError as e:\n",
    "        return {\"error\": f\"Validation error: {e.errors()}\"}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d927b43-ce41-447a-bffe-f76dadee243a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TP53', 'EGFR', 'MYC', 'BRCA1', 'CDK2']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ValidationError' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 5\u001b[0m, in \u001b[0;36menrichr_api\u001b[1;34m(input_data)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 5\u001b[0m     enr \u001b[38;5;241m=\u001b[39m \u001b[43mgp\u001b[49m\u001b[38;5;241m.\u001b[39menrichr(\n\u001b[0;32m      6\u001b[0m         gene_list\u001b[38;5;241m=\u001b[39minput_data,\n\u001b[0;32m      7\u001b[0m         gene_sets\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKEGG_2016\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m         outdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest/enrichr_kegg_tumor\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m     )\n\u001b[0;32m     10\u001b[0m     enrichment_results \u001b[38;5;241m=\u001b[39m enr\u001b[38;5;241m.\u001b[39mres2d\u001b[38;5;241m.\u001b[39mhead()\u001b[38;5;241m.\u001b[39mto_dict(orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gp' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m gene_list\u001b[38;5;241m=\u001b[39mtest_genes\n\u001b[1;32m----> 2\u001b[0m \u001b[43menrichr_api\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgene_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[29], line 12\u001b[0m, in \u001b[0;36menrichr_api\u001b[1;34m(input_data)\u001b[0m\n\u001b[0;32m     10\u001b[0m     enrichment_results \u001b[38;5;241m=\u001b[39m enr\u001b[38;5;241m.\u001b[39mres2d\u001b[38;5;241m.\u001b[39mhead()\u001b[38;5;241m.\u001b[39mto_dict(orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menrichment_results\u001b[39m\u001b[38;5;124m\"\u001b[39m: enrichment_results}\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[43mValidationError\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39merrors()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ValidationError' is not defined"
     ]
    }
   ],
   "source": [
    "gene_list=test_genes\n",
    "enrichr_api(gene_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883219f9-17f4-4b87-9244-c3cbe171cda1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "venv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
