{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da394f9c-9974-4e5d-8b55-e4c80acda7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "from agents import Agent, Runner\n",
    "from IPython.display import display, Markdown\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from bs4 import BeautifulSoup\n",
    "API_KEY = ''\n",
    "os.environ['OPENAI_API_KEY']=API_KEY\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7189bab1-8f4d-468a-85cf-021fd14bb2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from typing import TypedDict, List, Dict, Any\n",
    "from bs4 import BeautifulSoup\n",
    "from geofetch import Geofetcher\n",
    "from openai import OpenAI\n",
    "from langgraph.graph import StateGraph\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# ----------------------------------------\n",
    "# Agent state schema\n",
    "# ----------------------------------------\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    query_gene: str\n",
    "    pathway_info: Dict[str, Any]\n",
    "    metadata: Dict[str, Any]\n",
    "    gse_list: List[str]\n",
    "    research_plan: str\n",
    "\n",
    "# ----------------------------------------\n",
    "# Utility functions (LLM + GEO search)\n",
    "# ----------------------------------------\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def extract_json_block(text):\n",
    "    match = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", text, re.DOTALL)\n",
    "    if match:\n",
    "        return json.loads(match.group(1))\n",
    "    else:\n",
    "        return json.loads(text)\n",
    "\n",
    "def generate_pathway_info(query_gene, model=\"gpt-4o-mini\", temperature=0.3):\n",
    "    prompt = f\"\"\"\n",
    "You are a biomedical assistant.\n",
    "\n",
    "Given the gene {query_gene}, return:\n",
    "1. Key gene symbols in the same biological pathway.\n",
    "2. Drugs or compounds that inhibit this pathway or {query_gene}'s activity.\n",
    "3. The disease areas or biological processes this pathway is involved in.\n",
    "\n",
    "Provide answers in JSON format with keys: \"genes\", \"drugs\", \"pathways\"\n",
    "    \"\"\"\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    content = completion.choices[0].message.content\n",
    "    return extract_json_block(content)\n",
    "\n",
    "def scrape_organism_from_geo_html(geo_accession):\n",
    "    url = f\"https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc={geo_accession}\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        organism_row = soup.find(\"td\", string=\"Organism\")\n",
    "        if organism_row and organism_row.find_next_sibling(\"td\"):\n",
    "            return organism_row.find_next_sibling(\"td\").text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Failed to scrape organism for {geo_accession}: {e}\")\n",
    "    return \"Unknown\"\n",
    "\n",
    "def search_geo_datasets(keyword, retmax=10):\n",
    "    base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "    params = {\n",
    "        \"db\": \"gds\",\n",
    "        \"term\": keyword,\n",
    "        \"retmode\": \"json\",\n",
    "        \"retmax\": retmax\n",
    "    }\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()\n",
    "    return response.json().get(\"esearchresult\", {}).get(\"idlist\", [])\n",
    "\n",
    "def fetch_gse_accessions(id_list):\n",
    "    if not id_list:\n",
    "        return {}\n",
    "    base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi\"\n",
    "    params = {\n",
    "        \"db\": \"gds\",\n",
    "        \"id\": \",\".join(id_list),\n",
    "        \"retmode\": \"json\"\n",
    "    }\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()\n",
    "    summaries = response.json().get(\"result\", {})\n",
    "    summaries.pop(\"uids\", None)\n",
    "    gse_dict = {}\n",
    "    for uid, info in summaries.items():\n",
    "        accession = info.get(\"accession\")\n",
    "        title = info.get(\"title\")\n",
    "        link = f\"https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc={accession}\"\n",
    "        organism = scrape_organism_from_geo_html(accession)\n",
    "        gse_dict[accession] = {\n",
    "            \"title\": title,\n",
    "            \"organism\": organism,\n",
    "            \"link\": link\n",
    "        }\n",
    "    return gse_dict\n",
    "\n",
    "def search_geo_items(item_list, label=\"gene\", max_results=5):\n",
    "    results = {}\n",
    "    for item in item_list:\n",
    "        query = f\"{item} AND rna-seq\"\n",
    "        print(f\"🔍 Searching GEO for {label}: {item}\")\n",
    "        ids = search_geo_datasets(query, retmax=max_results)\n",
    "        datasets = fetch_gse_accessions(ids)\n",
    "        results[item] = datasets\n",
    "        time.sleep(0.3)\n",
    "    return results\n",
    "\n",
    "def get_geofetch_projects(gse_list, metadata_folder=\"geofetch_metadata\"):\n",
    "    geof = Geofetcher(\n",
    "        processed=True,\n",
    "        acc_anno=True,\n",
    "        discard_soft=True,\n",
    "        metadata_folder=metadata_folder\n",
    "    )\n",
    "    projects = {}\n",
    "    for gse in gse_list:\n",
    "        try:\n",
    "            print(f\"📥 Fetching metadata for {gse}\")\n",
    "            result = geof.get_projects(gse)\n",
    "            projects.update(result)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to fetch {gse}: {e}\")\n",
    "    return projects\n",
    "\n",
    "import subprocess\n",
    "def download_processed_files_via_cli(gse_list, output_dir=\"geofetch_metadata\", overwrite=False):\n",
    "    for gse in gse_list:\n",
    "        gse_path = os.path.join(output_dir, gse)\n",
    "        if os.path.exists(gse_path) and not overwrite:\n",
    "            print(f\"✅ {gse}: already exists at {gse_path}, skipping.\")\n",
    "            continue\n",
    "        try:\n",
    "            subprocess.run(\n",
    "                [\"geofetch\", \"-i\", gse, \"--processed\", \"-m\", output_dir],\n",
    "                check=True\n",
    "            )\n",
    "            print(f\"✅ Finished downloading for {gse}\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"❌ geofetch failed for {gse}: {e}\")\n",
    "\n",
    "def save_combined_metadata_csv_from_state(state: Dict, csv_path: str = \"geofetch_metadata/combined_metadata.csv\") -> pd.DataFrame:\n",
    "    metadata = state.get(\"metadata\", {})\n",
    "    if not metadata:\n",
    "        raise ValueError(\"No metadata found in the agent state.\")\n",
    "\n",
    "    all_dfs = []\n",
    "    for gse, project in metadata.items():\n",
    "        try:\n",
    "            df = project.sample_table.copy()\n",
    "            df[\"source_gse\"] = gse\n",
    "            all_dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Failed to extract sample_table from {gse}: {e}\")\n",
    "\n",
    "    if not all_dfs:\n",
    "        raise ValueError(\"No sample tables to save.\")\n",
    "\n",
    "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
    "    combined_df.to_csv(csv_path, index=False)\n",
    "    print(f\"✅ Combined metadata saved to: {os.path.abspath(csv_path)}\")\n",
    "    return combined_df\n",
    "\n",
    "# ----------------------------------------\n",
    "# Agent 1: Ingestor node\n",
    "# ----------------------------------------\n",
    "\n",
    "def ingest_and_prepare(state: dict) -> dict:\n",
    "    query_gene = state[\"query_gene\"]\n",
    "    pathway_info = generate_pathway_info(query_gene)\n",
    "    gene_list = pathway_info.get(\"genes\", [])\n",
    "    drug_list = pathway_info.get(\"drugs\", [])\n",
    "\n",
    "    gene_results = search_geo_items(gene_list, label=\"gene\")\n",
    "    drug_results = search_geo_items(drug_list, label=\"drug\")\n",
    "\n",
    "    all_gse = set()\n",
    "    for r in [gene_results, drug_results]:\n",
    "        for v in r.values():\n",
    "            all_gse.update(v.keys())\n",
    "    all_gse = list(all_gse)\n",
    "\n",
    "    metadata = get_geofetch_projects(all_gse, metadata_folder=\"geofetch_metadata\")\n",
    "    download_processed_files_via_cli(all_gse, output_dir=\"geofetch_metadata\")\n",
    "\n",
    "    try:\n",
    "        save_combined_metadata_csv_from_state({\"metadata\": metadata})\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Failed to save metadata CSV: {e}\")\n",
    "\n",
    "    return {\n",
    "        \"query_gene\": query_gene,\n",
    "        \"pathway_info\": pathway_info,\n",
    "        \"metadata\": metadata,\n",
    "        \"gse_list\": all_gse\n",
    "    }\n",
    "\n",
    "# ----------------------------------------\n",
    "# Agent 2: Analyst node\n",
    "# ----------------------------------------\n",
    "\n",
    "def analyze_metadata_and_plan(state: AgentState) -> AgentState:\n",
    "    metadata = state[\"metadata\"]\n",
    "    drug_list = state[\"pathway_info\"].get(\"drugs\", [])\n",
    "    query_gene = state[\"query_gene\"]\n",
    "    selected = []\n",
    "\n",
    "    for gse, project in metadata.items():\n",
    "        df = project.sample_table\n",
    "        if \"processed_file_ftp\" in df.columns and df[\"processed_file_ftp\"].notna().any():\n",
    "            if any(drug.lower() in df.to_string().lower() for drug in drug_list):\n",
    "                selected.append((gse, df.shape[0]))\n",
    "\n",
    "    plan = f\"🧬 Research Plan for {query_gene} and drugs {drug_list}:\\n\"\n",
    "    if not selected:\n",
    "        plan += \"No relevant processed datasets were found.\\n\"\n",
    "    else:\n",
    "        plan += f\"{len(selected)} datasets selected:\\n\"\n",
    "        for gse, n in selected:\n",
    "            plan += f\"  - {gse} ({n} samples)\\n\"\n",
    "        plan += \"\\nNext: perform differential expression and gene signature clustering.\"\n",
    "\n",
    "    return {**state, \"research_plan\": plan}\n",
    "\n",
    "\n",
    "def analyze_metadata_and_plan(state: AgentState) -> AgentState:\n",
    "    import pandas as pd\n",
    "    import os\n",
    "\n",
    "    csv_path = \"geofetch_metadata/combined_metadata.csv\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"❌ Metadata CSV not found at {csv_path}\")\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Ensure required columns exist\n",
    "    required_columns = {\"sample_name\", \"sample_source_name_ch1\", \"sample_title\"}\n",
    "    if not required_columns.issubset(df.columns):\n",
    "        raise ValueError(f\"❌ Metadata CSV must include columns: {required_columns}\")\n",
    "\n",
    "    # Take first 50 rows for LLM context\n",
    "    selected_df = df[list(required_columns)].fillna(\"\").head(50)\n",
    "    table_preview = selected_df.to_markdown(index=False)\n",
    "\n",
    "    query_gene = state[\"query_gene\"]\n",
    "    drug_list = state[\"pathway_info\"].get(\"drugs\", [])\n",
    "    selected_df = df[[\"sample_name\", \"sample_source_name_ch1\", \"sample_title\"]].fillna(\"\").head(50)\n",
    "    table_preview = selected_df.to_markdown(index=False)\n",
    "    print(\"🧪 Table preview sent to LLM:\\n\", table_preview)  # ✅ Add this line\n",
    "\n",
    "    # Build LLM prompt\n",
    "    prompt = f\"\"\"\n",
    "You are a biomedical research assistant.\n",
    "\n",
    "The target gene is **{query_gene}**, and the related drugs of interest are: {', '.join(drug_list)}.\n",
    "\n",
    "Below is a preview of sample metadata (first 50 rows) from multiple GEO studies. Each row includes:\n",
    "- sample name\n",
    "- sample source (cell line, tissue)\n",
    "- sample title (may indicate treatment or condition)\n",
    "\n",
    "Your task:\n",
    "1. Identify which studies include drug-treated samples.\n",
    "2. Identify the control groups if available.\n",
    "3. Determine the sample types (e.g., cell lines or tissues).\n",
    "4. Recommend studies and sample comparisons suitable for differential gene expression and drug-response signature analysis.\n",
    "\n",
    "Respond with:\n",
    "- GSE or study names (if known)\n",
    "- The experimental comparison design\n",
    "- Why the dataset is suitable (or not)\n",
    "- Bullet points summarizing each recommended comparison\n",
    "\n",
    "Sample Metadata Table:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # Call LLM\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0.3,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    research_plan = response.choices[0].message.content.strip()\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"research_plan\": research_plan\n",
    "    }\n",
    "\n",
    "###\n",
    "def analyze_metadata_and_plan(state: AgentState) -> AgentState:\n",
    "    import pandas as pd\n",
    "    import os\n",
    "\n",
    "    csv_path = \"geofetch_metadata/combined_metadata.csv\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"❌ Metadata CSV not found at {csv_path}\")\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Ensure required columns are available\n",
    "    required_cols = [\"gse\", \"sample_name\", \"sample_title\", \"sample_source_name_ch1\", \"sample_geo_accession\"]\n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        raise ValueError(f\"❌ Metadata CSV must contain the following columns: {required_cols}\")\n",
    "\n",
    "    # Clean and preview first 30 rows\n",
    "    preview_df = df[required_cols].fillna(\"\").head(30)\n",
    "    preview_text = preview_df.to_string(index=False)\n",
    "\n",
    "    query_gene = state[\"query_gene\"]\n",
    "    drug_list = state[\"pathway_info\"].get(\"drugs\", [])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a biomedical research assistant.\n",
    "\n",
    "The target gene is **{query_gene}** and the related drugs of interest are: {', '.join(drug_list)}.\n",
    "\n",
    "Below is a preview of sample metadata from several GEO datasets.\n",
    "Each row includes:\n",
    "- GSE accession\n",
    "- Sample name\n",
    "- Sample title (may contain treatment or control info)\n",
    "- Sample source (cell type or tissue)\n",
    "- Sample GEO accession\n",
    "\n",
    "Sample Metadata Table:\n",
    "{preview_text}\n",
    "\n",
    "Based on the sample names, titles, and sources:\n",
    "1. Which GSE studies contain drug-treated samples and matching control groups?\n",
    "2. What cell types or tissues are used?\n",
    "3. Which treatments are applied? What are the controls?\n",
    "4. Recommend GSEs and sample pairs suitable for differential gene expression to identify drug-response gene signatures.\n",
    "\n",
    "Be specific, refer to GSE and sample names where possible, and explain why you recommend them.\n",
    "\"\"\"\n",
    "\n",
    "    # Call OpenAI LLM\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0.3,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    research_plan = response.choices[0].message.content.strip()\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"research_plan\": research_plan\n",
    "    }\n",
    "\n",
    "# ----------------------------------------\n",
    "# LangGraph pipeline\n",
    "# ----------------------------------------\n",
    "\n",
    "workflow = StateGraph(state_schema=AgentState)\n",
    "workflow.add_node(\"Ingestor\", RunnableLambda(ingest_and_prepare))\n",
    "workflow.add_node(\"Analyst\", RunnableLambda(analyze_metadata_and_plan))\n",
    "workflow.set_entry_point(\"Ingestor\")\n",
    "workflow.add_edge(\"Ingestor\", \"Analyst\")\n",
    "workflow.set_finish_point(\"Analyst\")\n",
    "graph = workflow.compile()\n",
    "\n",
    "# ----------------------------------------\n",
    "# Invoke the graph\n",
    "# ----------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6f86e31-7ce2-4491-bca7-2655eb794cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4669ebdb-55b6-4699-a95c-1e90468295a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Searching GEO for gene: PTGS2\n",
      "🔍 Searching GEO for gene: PTGS1\n",
      "🔍 Searching GEO for gene: ALOX5\n",
      "🔍 Searching GEO for gene: CYP2C9\n",
      "🔍 Searching GEO for gene: CYP2C19\n",
      "🔍 Searching GEO for drug: Aspirin\n",
      "🔍 Searching GEO for drug: Ibuprofen\n",
      "🔍 Searching GEO for drug: Naproxen\n",
      "🔍 Searching GEO for drug: Celecoxib\n",
      "🔍 Searching GEO for drug: Diclofenac\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:23]\u001b[0m Metadata folder: C:\\Users\\difen\\POPPER\\geofetch_metadata\\project_name\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:23]\u001b[0m Trying GSE278083 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:23]\u001b[0m \u001b[38;5;200mProcessing accession 1 of 1: 'GSE278083'\u001b[0m\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:23]\u001b[0m Trying GSE278083 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:23]\u001b[0m Skipped 0 accessions. Starting now.\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:23]\u001b[0m \u001b[38;5;200mProcessing accession 1 of 1: 'GSE278083'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Fetching metadata for GSE278083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:25]\u001b[0m Total number of processed SERIES files found is: 4\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:25]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:25]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:25]\u001b[0m Finished processing 1 accession(s)\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:25]\u001b[0m Cleaning soft files ...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:25]\u001b[0m No files found. No data to save. File geofetch_metadata\\project_name\\GSE278083_samples\\GSE278083_samples.csv won't be created\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:25]\u001b[0m Trying GSE94840 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:25]\u001b[0m Trying GSE94840 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:25]\u001b[0m Skipped 0 accessions. Starting now.\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:25]\u001b[0m \u001b[38;5;200mProcessing accession 1 of 1: 'GSE94840'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Fetching metadata for GSE94840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:26]\u001b[0m Total number of processed SERIES files found is: 1\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:26]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:26]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:26]\u001b[0m Finished processing 1 accession(s)\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:26]\u001b[0m Cleaning soft files ...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:26]\u001b[0m No files found. No data to save. File geofetch_metadata\\project_name\\GSE94840_samples\\GSE94840_samples.csv won't be created\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:26]\u001b[0m Trying GSE244787 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:26]\u001b[0m Trying GSE244787 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:26]\u001b[0m Skipped 0 accessions. Starting now.\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:26]\u001b[0m \u001b[38;5;200mProcessing accession 1 of 1: 'GSE244787'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Fetching metadata for GSE244787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:30]\u001b[0m Total number of processed SERIES files found is: 1\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:30]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:30]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:30]\u001b[0m Finished processing 1 accession(s)\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:30]\u001b[0m Cleaning soft files ...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:30]\u001b[0m No files found. No data to save. File geofetch_metadata\\project_name\\GSE244787_samples\\GSE244787_samples.csv won't be created\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:30]\u001b[0m Trying GSE110293 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:30]\u001b[0m Trying GSE110293 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:30]\u001b[0m Skipped 0 accessions. Starting now.\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:30]\u001b[0m \u001b[38;5;200mProcessing accession 1 of 1: 'GSE110293'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Fetching metadata for GSE110293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:32]\u001b[0m \n",
      "Total number of processed SAMPLES files found is: 24\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:32]\u001b[0m Total number of processed SERIES files found is: 0\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:32]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:32]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:32]\u001b[0m Finished processing 1 accession(s)\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:32]\u001b[0m Cleaning soft files ...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:32]\u001b[0m Unifying and saving of metadata... \n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:32]\u001b[0m Trying GSE101766 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:32]\u001b[0m Trying GSE101766 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:32]\u001b[0m Skipped 0 accessions. Starting now.\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:32]\u001b[0m \u001b[38;5;200mProcessing accession 1 of 1: 'GSE101766'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Fetching metadata for GSE101766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:44]\u001b[0m Total number of processed SERIES files found is: 1\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:44]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:44]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:44]\u001b[0m Finished processing 1 accession(s)\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:44]\u001b[0m Cleaning soft files ...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:44]\u001b[0m No files found. No data to save. File geofetch_metadata\\project_name\\GSE101766_samples\\GSE101766_samples.csv won't be created\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:44]\u001b[0m Trying GSE277028 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:44]\u001b[0m Trying GSE277028 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:44]\u001b[0m Skipped 0 accessions. Starting now.\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:44]\u001b[0m \u001b[38;5;200mProcessing accession 1 of 1: 'GSE277028'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Fetching metadata for GSE277028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:48]\u001b[0m Total number of processed SERIES files found is: 2\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:48]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:48]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:48]\u001b[0m Finished processing 1 accession(s)\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:48]\u001b[0m Cleaning soft files ...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:48]\u001b[0m No files found. No data to save. File geofetch_metadata\\project_name\\GSE277028_samples\\GSE277028_samples.csv won't be created\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:48]\u001b[0m Trying GSE180857 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:48]\u001b[0m Trying GSE180857 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:48]\u001b[0m Skipped 0 accessions. Starting now.\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:48]\u001b[0m \u001b[38;5;200mProcessing accession 1 of 1: 'GSE180857'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Fetching metadata for GSE180857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:52]\u001b[0m \n",
      "Total number of processed SAMPLES files found is: 31\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:52]\u001b[0m Total number of processed SERIES files found is: 0\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:52]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:52]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:52]\u001b[0m Finished processing 1 accession(s)\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:52]\u001b[0m Cleaning soft files ...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:52]\u001b[0m Unifying and saving of metadata... \n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:52]\u001b[0m Trying GSE231460 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:52]\u001b[0m Trying GSE231460 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:52]\u001b[0m Skipped 0 accessions. Starting now.\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:52]\u001b[0m \u001b[38;5;200mProcessing accession 1 of 1: 'GSE231460'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Fetching metadata for GSE231460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:53]\u001b[0m Total number of processed SERIES files found is: 1\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:53]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:53]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:53]\u001b[0m Finished processing 1 accession(s)\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:53]\u001b[0m Cleaning soft files ...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:53]\u001b[0m No files found. No data to save. File geofetch_metadata\\project_name\\GSE231460_samples\\GSE231460_samples.csv won't be created\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:53]\u001b[0m Trying GSE97066 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:53]\u001b[0m Trying GSE97066 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:53]\u001b[0m Skipped 0 accessions. Starting now.\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:53]\u001b[0m \u001b[38;5;200mProcessing accession 1 of 1: 'GSE97066'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Fetching metadata for GSE97066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:59]\u001b[0m Total number of processed SERIES files found is: 1\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:59]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:59]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:59]\u001b[0m Finished processing 1 accession(s)\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:59]\u001b[0m Cleaning soft files ...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:59]\u001b[0m No files found. No data to save. File geofetch_metadata\\project_name\\GSE97066_samples\\GSE97066_samples.csv won't be created\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:59]\u001b[0m Trying GSE144219 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:59]\u001b[0m Trying GSE144219 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:59]\u001b[0m Skipped 0 accessions. Starting now.\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:19:59]\u001b[0m \u001b[38;5;200mProcessing accession 1 of 1: 'GSE144219'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Fetching metadata for GSE144219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:20:52]\u001b[0m Total number of processed SERIES files found is: 5\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:20:52]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:20:52]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:20:52]\u001b[0m Finished processing 1 accession(s)\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:20:52]\u001b[0m Cleaning soft files ...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:20:52]\u001b[0m No files found. No data to save. File geofetch_metadata\\project_name\\GSE144219_samples\\GSE144219_samples.csv won't be created\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:20:52]\u001b[0m Trying GSE131732 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:20:52]\u001b[0m Trying GSE131732 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:20:52]\u001b[0m Skipped 0 accessions. Starting now.\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:20:52]\u001b[0m \u001b[38;5;200mProcessing accession 1 of 1: 'GSE131732'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Fetching metadata for GSE131732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:20:54]\u001b[0m Total number of processed SERIES files found is: 1\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:20:54]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:20:54]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:20:54]\u001b[0m Finished processing 1 accession(s)\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:20:54]\u001b[0m Cleaning soft files ...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:20:54]\u001b[0m No files found. No data to save. File geofetch_metadata\\project_name\\GSE131732_samples\\GSE131732_samples.csv won't be created\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:20:54]\u001b[0m Trying GSE95802 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:20:54]\u001b[0m Trying GSE95802 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:20:54]\u001b[0m Skipped 0 accessions. Starting now.\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:20:54]\u001b[0m \u001b[38;5;200mProcessing accession 1 of 1: 'GSE95802'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Fetching metadata for GSE95802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:20:58]\u001b[0m Total number of processed SERIES files found is: 1\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:20:58]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:20:58]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:20:58]\u001b[0m Finished processing 1 accession(s)\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:20:58]\u001b[0m Cleaning soft files ...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:20:58]\u001b[0m No files found. No data to save. File geofetch_metadata\\project_name\\GSE95802_samples\\GSE95802_samples.csv won't be created\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:20:58]\u001b[0m Trying GSE222593 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:20:58]\u001b[0m Trying GSE222593 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:20:58]\u001b[0m Skipped 0 accessions. Starting now.\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:20:58]\u001b[0m \u001b[38;5;200mProcessing accession 1 of 1: 'GSE222593'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Fetching metadata for GSE222593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:06]\u001b[0m \n",
      "Total number of processed SAMPLES files found is: 355\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:06]\u001b[0m Total number of processed SERIES files found is: 6\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:06]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:06]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:06]\u001b[0m Finished processing 1 accession(s)\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:06]\u001b[0m Cleaning soft files ...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:06]\u001b[0m Unifying and saving of metadata... \n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:07]\u001b[0m Trying GSE110282 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:07]\u001b[0m Trying GSE110282 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:07]\u001b[0m Skipped 0 accessions. Starting now.\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:07]\u001b[0m \u001b[38;5;200mProcessing accession 1 of 1: 'GSE110282'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Fetching metadata for GSE110282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:08]\u001b[0m Total number of processed SERIES files found is: 1\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:08]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:08]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:08]\u001b[0m Finished processing 1 accession(s)\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:08]\u001b[0m Cleaning soft files ...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:08]\u001b[0m No files found. No data to save. File geofetch_metadata\\project_name\\GSE110282_samples\\GSE110282_samples.csv won't be created\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:08]\u001b[0m Trying GSE279800 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:08]\u001b[0m Trying GSE279800 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:08]\u001b[0m Skipped 0 accessions. Starting now.\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:08]\u001b[0m \u001b[38;5;200mProcessing accession 1 of 1: 'GSE279800'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Fetching metadata for GSE279800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:11]\u001b[0m \n",
      "Total number of processed SAMPLES files found is: 8\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:11]\u001b[0m Total number of processed SERIES files found is: 0\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:11]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:11]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:11]\u001b[0m Finished processing 1 accession(s)\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:11]\u001b[0m Cleaning soft files ...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:11]\u001b[0m Unifying and saving of metadata... \n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:11]\u001b[0m Trying GSE279268 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:11]\u001b[0m Trying GSE279268 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:11]\u001b[0m Skipped 0 accessions. Starting now.\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:11]\u001b[0m \u001b[38;5;200mProcessing accession 1 of 1: 'GSE279268'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Fetching metadata for GSE279268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:17]\u001b[0m Total number of processed SERIES files found is: 8\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:17]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:17]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:17]\u001b[0m Finished processing 1 accession(s)\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:17]\u001b[0m Cleaning soft files ...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:17]\u001b[0m No files found. No data to save. File geofetch_metadata\\project_name\\GSE279268_samples\\GSE279268_samples.csv won't be created\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:17]\u001b[0m Trying GSE139044 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:17]\u001b[0m Trying GSE139044 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:17]\u001b[0m Skipped 0 accessions. Starting now.\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:17]\u001b[0m \u001b[38;5;200mProcessing accession 1 of 1: 'GSE139044'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Fetching metadata for GSE139044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:18]\u001b[0m Total number of processed SERIES files found is: 1\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:18]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:18]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:18]\u001b[0m Finished processing 1 accession(s)\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:18]\u001b[0m Cleaning soft files ...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:18]\u001b[0m No files found. No data to save. File geofetch_metadata\\project_name\\GSE139044_samples\\GSE139044_samples.csv won't be created\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:18]\u001b[0m Trying GSE139045 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:18]\u001b[0m Trying GSE139045 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:18]\u001b[0m Skipped 0 accessions. Starting now.\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:18]\u001b[0m \u001b[38;5;200mProcessing accession 1 of 1: 'GSE139045'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Fetching metadata for GSE139045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:19]\u001b[0m Total number of processed SERIES files found is: 1\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:19]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:19]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:19]\u001b[0m Finished processing 1 accession(s)\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:19]\u001b[0m Cleaning soft files ...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:20]\u001b[0m No files found. No data to save. File geofetch_metadata\\project_name\\GSE139045_samples\\GSE139045_samples.csv won't be created\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:20]\u001b[0m Trying GSE286021 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:20]\u001b[0m Trying GSE286021 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:20]\u001b[0m Skipped 0 accessions. Starting now.\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:20]\u001b[0m \u001b[38;5;200mProcessing accession 1 of 1: 'GSE286021'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Fetching metadata for GSE286021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:22]\u001b[0m Total number of processed SERIES files found is: 1\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:22]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:22]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:22]\u001b[0m Finished processing 1 accession(s)\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:22]\u001b[0m Cleaning soft files ...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:22]\u001b[0m No files found. No data to save. File geofetch_metadata\\project_name\\GSE286021_samples\\GSE286021_samples.csv won't be created\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:22]\u001b[0m Trying GSE281885 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:22]\u001b[0m Trying GSE281885 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:22]\u001b[0m Skipped 0 accessions. Starting now.\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:22]\u001b[0m \u001b[38;5;200mProcessing accession 1 of 1: 'GSE281885'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Fetching metadata for GSE281885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:24]\u001b[0m \n",
      "Total number of processed SAMPLES files found is: 10\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:24]\u001b[0m Total number of processed SERIES files found is: 0\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:24]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:24]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:24]\u001b[0m Finished processing 1 accession(s)\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:24]\u001b[0m Cleaning soft files ...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:24]\u001b[0m Unifying and saving of metadata... \n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:24]\u001b[0m Trying GSE124074 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:24]\u001b[0m Trying GSE124074 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:24]\u001b[0m Skipped 0 accessions. Starting now.\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:24]\u001b[0m \u001b[38;5;200mProcessing accession 1 of 1: 'GSE124074'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Fetching metadata for GSE124074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:31]\u001b[0m \n",
      "Total number of processed SAMPLES files found is: 60\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:31]\u001b[0m Total number of processed SERIES files found is: 1\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:31]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:31]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:31]\u001b[0m Finished processing 1 accession(s)\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:31]\u001b[0m Cleaning soft files ...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:31]\u001b[0m Unifying and saving of metadata... \n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:31]\u001b[0m Trying GSE38809 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:31]\u001b[0m Trying GSE38809 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:31]\u001b[0m Skipped 0 accessions. Starting now.\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:31]\u001b[0m \u001b[38;5;200mProcessing accession 1 of 1: 'GSE38809'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Fetching metadata for GSE38809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:34]\u001b[0m \n",
      "Total number of processed SAMPLES files found is: 3\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:34]\u001b[0m Total number of processed SERIES files found is: 3\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:34]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:34]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:34]\u001b[0m Finished processing 1 accession(s)\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:34]\u001b[0m Cleaning soft files ...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:34]\u001b[0m Unifying and saving of metadata... \n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:34]\u001b[0m Trying GSE184884 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:34]\u001b[0m Trying GSE184884 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:34]\u001b[0m Skipped 0 accessions. Starting now.\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:34]\u001b[0m \u001b[38;5;200mProcessing accession 1 of 1: 'GSE184884'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Fetching metadata for GSE184884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:43]\u001b[0m \n",
      "Total number of processed SAMPLES files found is: 197\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:43]\u001b[0m Total number of processed SERIES files found is: 3\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:43]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:43]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:43]\u001b[0m Finished processing 1 accession(s)\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:43]\u001b[0m Cleaning soft files ...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:43]\u001b[0m Unifying and saving of metadata... \n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:43]\u001b[0m Trying GSE245768 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:43]\u001b[0m Trying GSE245768 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:43]\u001b[0m Skipped 0 accessions. Starting now.\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:43]\u001b[0m \u001b[38;5;200mProcessing accession 1 of 1: 'GSE245768'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Fetching metadata for GSE245768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:46]\u001b[0m \n",
      "Total number of processed SAMPLES files found is: 15\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:46]\u001b[0m Total number of processed SERIES files found is: 0\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:46]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:46]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:46]\u001b[0m Finished processing 1 accession(s)\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:46]\u001b[0m Cleaning soft files ...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:46]\u001b[0m Unifying and saving of metadata... \n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:46]\u001b[0m Trying GSE242369 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:46]\u001b[0m Trying GSE242369 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:46]\u001b[0m Skipped 0 accessions. Starting now.\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:46]\u001b[0m \u001b[38;5;200mProcessing accession 1 of 1: 'GSE242369'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Fetching metadata for GSE242369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:50]\u001b[0m Total number of processed SERIES files found is: 1\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:50]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:50]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:50]\u001b[0m Finished processing 1 accession(s)\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:50]\u001b[0m Cleaning soft files ...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:50]\u001b[0m No files found. No data to save. File geofetch_metadata\\project_name\\GSE242369_samples\\GSE242369_samples.csv won't be created\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:50]\u001b[0m Trying GSE263024 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:50]\u001b[0m Trying GSE263024 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:50]\u001b[0m Skipped 0 accessions. Starting now.\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:50]\u001b[0m \u001b[38;5;200mProcessing accession 1 of 1: 'GSE263024'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Fetching metadata for GSE263024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:51]\u001b[0m Total number of processed SERIES files found is: 2\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:51]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:51]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:51]\u001b[0m Finished processing 1 accession(s)\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:51]\u001b[0m Cleaning soft files ...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:51]\u001b[0m No files found. No data to save. File geofetch_metadata\\project_name\\GSE263024_samples\\GSE263024_samples.csv won't be created\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:51]\u001b[0m Trying GSE255683 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:51]\u001b[0m Trying GSE255683 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:51]\u001b[0m Skipped 0 accessions. Starting now.\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:51]\u001b[0m \u001b[38;5;200mProcessing accession 1 of 1: 'GSE255683'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Fetching metadata for GSE255683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:53]\u001b[0m \n",
      "Total number of processed SAMPLES files found is: 20\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:53]\u001b[0m Total number of processed SERIES files found is: 0\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:53]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:53]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:53]\u001b[0m Finished processing 1 accession(s)\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:53]\u001b[0m Cleaning soft files ...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:53]\u001b[0m Unifying and saving of metadata... \n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:53]\u001b[0m Trying GSE175744 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:53]\u001b[0m Trying GSE175744 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:53]\u001b[0m Skipped 0 accessions. Starting now.\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:53]\u001b[0m \u001b[38;5;200mProcessing accession 1 of 1: 'GSE175744'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Fetching metadata for GSE175744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:59]\u001b[0m Total number of processed SERIES files found is: 1\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:59]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:59]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:59]\u001b[0m Finished processing 1 accession(s)\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:59]\u001b[0m Cleaning soft files ...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:59]\u001b[0m No files found. No data to save. File geofetch_metadata\\project_name\\GSE175744_samples\\GSE175744_samples.csv won't be created\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:59]\u001b[0m Trying GSE95588 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:59]\u001b[0m Trying GSE95588 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:59]\u001b[0m Skipped 0 accessions. Starting now.\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:22:59]\u001b[0m \u001b[38;5;200mProcessing accession 1 of 1: 'GSE95588'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Fetching metadata for GSE95588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:03]\u001b[0m Total number of processed SERIES files found is: 2\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:03]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:03]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:03]\u001b[0m Finished processing 1 accession(s)\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:03]\u001b[0m Cleaning soft files ...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:03]\u001b[0m No files found. No data to save. File geofetch_metadata\\project_name\\GSE95588_samples\\GSE95588_samples.csv won't be created\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:03]\u001b[0m Trying GSE120596 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:03]\u001b[0m Trying GSE120596 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:03]\u001b[0m Skipped 0 accessions. Starting now.\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:03]\u001b[0m \u001b[38;5;200mProcessing accession 1 of 1: 'GSE120596'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Fetching metadata for GSE120596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:06]\u001b[0m Total number of processed SERIES files found is: 2\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:06]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:06]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:06]\u001b[0m Finished processing 1 accession(s)\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:06]\u001b[0m Cleaning soft files ...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:06]\u001b[0m No files found. No data to save. File geofetch_metadata\\project_name\\GSE120596_samples\\GSE120596_samples.csv won't be created\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:06]\u001b[0m Trying GSE162256 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:06]\u001b[0m Trying GSE162256 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:06]\u001b[0m Skipped 0 accessions. Starting now.\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:06]\u001b[0m \u001b[38;5;200mProcessing accession 1 of 1: 'GSE162256'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Fetching metadata for GSE162256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:28]\u001b[0m Total number of processed SERIES files found is: 1\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:28]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:28]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:28]\u001b[0m Finished processing 1 accession(s)\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:28]\u001b[0m Cleaning soft files ...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:28]\u001b[0m No files found. No data to save. File geofetch_metadata\\project_name\\GSE162256_samples\\GSE162256_samples.csv won't be created\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:28]\u001b[0m Trying GSE242272 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:28]\u001b[0m Trying GSE242272 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:28]\u001b[0m Skipped 0 accessions. Starting now.\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:28]\u001b[0m \u001b[38;5;200mProcessing accession 1 of 1: 'GSE242272'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Fetching metadata for GSE242272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:30]\u001b[0m \n",
      "Total number of processed SAMPLES files found is: 18\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:30]\u001b[0m Total number of processed SERIES files found is: 0\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:30]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:30]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:30]\u001b[0m Finished processing 1 accession(s)\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:30]\u001b[0m Cleaning soft files ...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:30]\u001b[0m Unifying and saving of metadata... \n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:30]\u001b[0m Trying GSE156453 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:30]\u001b[0m Trying GSE156453 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:30]\u001b[0m Skipped 0 accessions. Starting now.\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:30]\u001b[0m \u001b[38;5;200mProcessing accession 1 of 1: 'GSE156453'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Fetching metadata for GSE156453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:45]\u001b[0m Total number of processed SERIES files found is: 2\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:45]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:45]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:45]\u001b[0m Finished processing 1 accession(s)\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:45]\u001b[0m Cleaning soft files ...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:45]\u001b[0m No files found. No data to save. File geofetch_metadata\\project_name\\GSE156453_samples\\GSE156453_samples.csv won't be created\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:45]\u001b[0m Trying GSE262419 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:45]\u001b[0m Trying GSE262419 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:45]\u001b[0m Skipped 0 accessions. Starting now.\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:23:45]\u001b[0m \u001b[38;5;200mProcessing accession 1 of 1: 'GSE262419'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Fetching metadata for GSE262419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:24:17]\u001b[0m Trying GSE221957 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:24:17]\u001b[0m Trying GSE221957 (not a file) as accession...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:24:17]\u001b[0m Skipped 0 accessions. Starting now.\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:24:17]\u001b[0m \u001b[38;5;200mProcessing accession 1 of 1: 'GSE221957'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Failed to fetch GSE262419: Error in requesting file: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?targ=gsm&acc=GSE262419&form=text&view=full\n",
      "📥 Fetching metadata for GSE221957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:24:20]\u001b[0m \n",
      "Total number of processed SAMPLES files found is: 16\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:24:20]\u001b[0m Total number of processed SERIES files found is: 0\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:24:20]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:24:20]\u001b[0m Expanding metadata list...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:24:20]\u001b[0m Finished processing 1 accession(s)\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:24:20]\u001b[0m Cleaning soft files ...\n",
      "\u001b[1;30m[INFO]\u001b[0m \u001b[32m[23:24:20]\u001b[0m Unifying and saving of metadata... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished downloading for GSE278083\n",
      "✅ Finished downloading for GSE94840\n",
      "✅ Finished downloading for GSE244787\n",
      "❌ geofetch failed for GSE110293: Command '['geofetch', '-i', 'GSE110293', '--processed', '-m', 'geofetch_metadata']' returned non-zero exit status 1.\n",
      "✅ Finished downloading for GSE101766\n",
      "✅ Finished downloading for GSE277028\n",
      "❌ geofetch failed for GSE180857: Command '['geofetch', '-i', 'GSE180857', '--processed', '-m', 'geofetch_metadata']' returned non-zero exit status 1.\n",
      "✅ Finished downloading for GSE231460\n",
      "✅ Finished downloading for GSE97066\n",
      "✅ Finished downloading for GSE144219\n",
      "✅ Finished downloading for GSE131732\n",
      "✅ Finished downloading for GSE95802\n",
      "❌ geofetch failed for GSE222593: Command '['geofetch', '-i', 'GSE222593', '--processed', '-m', 'geofetch_metadata']' returned non-zero exit status 1.\n",
      "✅ Finished downloading for GSE110282\n",
      "❌ geofetch failed for GSE279800: Command '['geofetch', '-i', 'GSE279800', '--processed', '-m', 'geofetch_metadata']' returned non-zero exit status 1.\n",
      "✅ Finished downloading for GSE279268\n",
      "✅ Finished downloading for GSE139044\n",
      "✅ Finished downloading for GSE139045\n",
      "✅ Finished downloading for GSE286021\n",
      "❌ geofetch failed for GSE281885: Command '['geofetch', '-i', 'GSE281885', '--processed', '-m', 'geofetch_metadata']' returned non-zero exit status 1.\n",
      "❌ geofetch failed for GSE124074: Command '['geofetch', '-i', 'GSE124074', '--processed', '-m', 'geofetch_metadata']' returned non-zero exit status 1.\n",
      "❌ geofetch failed for GSE38809: Command '['geofetch', '-i', 'GSE38809', '--processed', '-m', 'geofetch_metadata']' returned non-zero exit status 1.\n",
      "❌ geofetch failed for GSE184884: Command '['geofetch', '-i', 'GSE184884', '--processed', '-m', 'geofetch_metadata']' returned non-zero exit status 1.\n",
      "❌ geofetch failed for GSE245768: Command '['geofetch', '-i', 'GSE245768', '--processed', '-m', 'geofetch_metadata']' returned non-zero exit status 1.\n",
      "✅ Finished downloading for GSE242369\n",
      "✅ Finished downloading for GSE263024\n",
      "❌ geofetch failed for GSE255683: Command '['geofetch', '-i', 'GSE255683', '--processed', '-m', 'geofetch_metadata']' returned non-zero exit status 1.\n",
      "✅ Finished downloading for GSE175744\n",
      "✅ Finished downloading for GSE95588\n",
      "✅ Finished downloading for GSE120596\n",
      "✅ Finished downloading for GSE162256\n",
      "❌ geofetch failed for GSE242272: Command '['geofetch', '-i', 'GSE242272', '--processed', '-m', 'geofetch_metadata']' returned non-zero exit status 1.\n",
      "✅ Finished downloading for GSE156453\n",
      "✅ Finished downloading for GSE262419\n",
      "❌ geofetch failed for GSE221957: Command '['geofetch', '-i', 'GSE221957', '--processed', '-m', 'geofetch_metadata']' returned non-zero exit status 1.\n",
      "✅ Combined metadata saved to: C:\\Users\\difen\\POPPER\\geofetch_metadata\\combined_metadata.csv\n",
      "\n",
      "📋 Final Research Plan:\n",
      "To identify suitable GEO datasets for studying the target gene **COX1** and its response to the drugs of interest (Aspirin, Ibuprofen, Naproxen, Celecoxib, Diclofenac), we need to analyze the provided metadata for drug-treated samples and their corresponding control groups. Below is a breakdown of the findings based on the provided metadata.\n",
      "\n",
      "### 1. GSE Studies with Drug-Treated Samples and Matching Control Groups\n",
      "\n",
      "From the provided metadata, only the GSE180857 dataset appears to have samples with treatment information in the sample titles. However, there is no explicit mention of control samples in the provided data. \n",
      "\n",
      "- **GSE180857**: Contains samples with treatment information:\n",
      "  - **Sample Names**: \n",
      "    - bursa_rct-001_degenerative\n",
      "    - bursa_rct-002_degenerative\n",
      "    - bursa_rct-003_traumatic\n",
      "    - bursa_rct-004_traumatic\n",
      "    - bursa_rct-006_traumatic\n",
      "    - bursa_rct-008_degenerative\n",
      "\n",
      "### 2. Cell Types or Tissues Used\n",
      "\n",
      "- **GSE110293**: All samples are from the **Liver**.\n",
      "- **GSE180857**: All samples are from **Bursa**.\n",
      "\n",
      "### 3. Treatments Applied and Controls\n",
      "\n",
      "- **GSE180857**: \n",
      "  - Treatments:\n",
      "    - RCT-001, RCT-002 (Degenerative)\n",
      "    - RCT-003, RCT-004, RCT-006 (Traumatic)\n",
      "  - Controls: The metadata does not explicitly mention control samples for these treatments, which is critical for differential expression analysis.\n",
      "\n",
      "### 4. Recommendations for Differential Gene Expression Analysis\n",
      "\n",
      "Given the analysis, the following recommendations can be made:\n",
      "\n",
      "- **GSE180857**: Although it has treatment groups, the lack of explicit control samples limits its utility for differential expression analysis. However, if controls can be identified or if they exist in the full dataset, this GSE could be valuable for studying responses to treatments in the Bursa tissue. \n",
      "\n",
      "- **GSE110293**: While this dataset does not mention treatments, it includes a large number of liver samples. If there are other datasets associated with GSE110293 that include drug treatments or if additional metadata can provide insight into treatment conditions, it could be worth exploring.\n",
      "\n",
      "### Summary of Recommendations\n",
      "\n",
      "1. **GSE180857** is recommended for further investigation, particularly if control samples can be identified. The presence of treatment information suggests potential for studying drug responses, especially if the treatments correspond to the drugs of interest.\n",
      "   \n",
      "2. **GSE110293** should be monitored for additional datasets or metadata that may indicate drug treatments, as the liver is a relevant tissue for COX1 expression and drug metabolism.\n",
      "\n",
      "For a robust analysis, it is essential to ensure that control samples are included in any differential expression study to accurately assess the effects of the treatments on COX1 expression and other related pathways.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    result = graph.invoke({\"query_gene\": \"COX1\"})\n",
    "    print(\"\\n📋 Final Research Plan:\")\n",
    "    print(result[\"research_plan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b712a6ad-8d10-4910-a21d-024edc473575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "596cb43d-2bdf-4e78-b15b-18a38a45bfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import tarfile\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "def download_and_extract_gse(gse_id=\"GSE242272\", base_dir=\"rna_seq_analysis\"):\n",
    "    \"\"\"\n",
    "    Download GEO supplementary file (.tar) for the given GSE ID and extract it.\n",
    "    Returns the path to the extracted folder.\n",
    "    \"\"\"\n",
    "    download_dir = os.path.join(base_dir, gse_id)\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "    url = f\"https://www.ncbi.nlm.nih.gov/geo/download/?acc={gse_id}&format=file\"\n",
    "    tar_path = os.path.join(download_dir, f\"{gse_id}_supplement.tar\")\n",
    "\n",
    "    print(f\"📦 Downloading: {url}\")\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        with open(tar_path, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        print(f\"✅ Downloaded to: {tar_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Download failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"🗂️ Extracting TAR file...\")\n",
    "    try:\n",
    "        with tarfile.open(tar_path, \"r:*\") as tar:\n",
    "            tar.extractall(path=download_dir)\n",
    "        print(f\"✅ Extracted contents to: {download_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Extraction failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    return download_dir\n",
    "\n",
    "\n",
    "def decompress_gz_files(root_dir: str):\n",
    "    \"\"\"\n",
    "    Recursively decompress all .txt.gz files in a directory.\n",
    "    \"\"\"\n",
    "    print(f\"🔍 Decompressing .txt.gz files under: {root_dir}\")\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(\".txt.gz\"):\n",
    "                gz_path = os.path.join(dirpath, filename)\n",
    "                txt_path = gz_path[:-3]  # Remove .gz extension\n",
    "                try:\n",
    "                    with gzip.open(gz_path, 'rb') as f_in:\n",
    "                        with open(txt_path, 'wb') as f_out:\n",
    "                            shutil.copyfileobj(f_in, f_out)\n",
    "                    print(f\"✅ Decompressed: {gz_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Failed to decompress {gz_path}: {e}\")\n",
    "\n",
    "\n",
    "def run_deg_analysis_from_gse(gse_id=\"GSE242272\", base_dir=\"rna_seq_analysis\"):\n",
    "    gse_dir = os.path.join(base_dir, gse_id)\n",
    "    output_csv = os.path.join(base_dir, f\"{gse_id}_deseq2_results.csv\")\n",
    "\n",
    "    # Step 1: Assemble count matrix\n",
    "    count_dfs = []\n",
    "    sample_conditions = []\n",
    "\n",
    "    for subdir in os.listdir(gse_dir):\n",
    "        subpath = os.path.join(gse_dir, subdir)\n",
    "        if os.path.isdir(subpath) and subdir.startswith(\"GSM\"):\n",
    "            for fname in os.listdir(subpath):\n",
    "                if fname.endswith(\".txt\"):\n",
    "                    file_path = os.path.join(subpath, fname)\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path, sep=\"\\t\", header=None, names=[\"gene\", subdir])\n",
    "                        df.set_index(\"gene\", inplace=True)\n",
    "                        count_dfs.append(df)\n",
    "\n",
    "                        # infer condition from folder name\n",
    "                        label = \"treated\" if any(x in subdir.lower() for x in [\"treated\", \"pge2\", \"caffeine\"]) else \"control\"\n",
    "                        sample_conditions.append((subdir, label))\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(f\"⚠️ Error reading {file_path}: {e}\")\n",
    "\n",
    "    if not count_dfs:\n",
    "        raise ValueError(\"❌ No valid count files found.\")\n",
    "\n",
    "    combined_counts = pd.concat(count_dfs, axis=1).fillna(0).astype(int)\n",
    "    # messege for combined_counts created:\n",
    "    print(\"✅ Count matrix assembled.\")\n",
    "    print(\"📐 Shape (genes × samples):\", combined_counts.shape)\n",
    "    print(\"🔍 Preview:\")\n",
    "    print(combined_counts.iloc[:5, :5])  # Show first 5 genes × 5 samples\n",
    "    # TEST: Save raw count matrix\n",
    "    combined_counts.to_csv(f\"{base_dir}/{gse_id}_raw_counts.csv\")\n",
    "    print(f\"🧬 Raw count matrix saved to: {base_dir}/{gse_id}_raw_counts.csv\")\n",
    "    sample_df = pd.DataFrame(sample_conditions, columns=[\"sample\", \"condition\"])\n",
    "    sample_df.set_index(\"sample\", inplace=True)\n",
    "    \n",
    "    # Step 2: DESeq2\n",
    "    dds = py_DESeq2(\n",
    "        count_matrix=combined_counts.T,\n",
    "        design_matrix=sample_df,\n",
    "        design_formula=\"~ condition\",\n",
    "        gene_column=\"gene\"\n",
    "    )\n",
    "\n",
    "    dds.run_deseq()\n",
    "    res = dds.get_deseq_result()\n",
    "    res_sorted = res.sort_values(\"padj\").dropna().head(50)\n",
    "    res_sorted.to_csv(output_csv, index=False)\n",
    "    print(f\"📄 DESeq2 results saved to: {output_csv}\")\n",
    "\n",
    "    return res_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0c17b359-1036-4c10-b924-9ac3fc2a2853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📁 rna_seq_analysis/GSE242272\n",
      "  📄 GSE242272_supplement.tar\n",
      "  📄 GSM7757588_CD8_24h_Vehicle1.tabular.txt\n",
      "  📄 GSM7757588_CD8_24h_Vehicle1.tabular.txt.gz\n",
      "  📄 GSM7757589_CD8_24h_Vehicle2.tabular.txt\n",
      "  📄 GSM7757589_CD8_24h_Vehicle2.tabular.txt.gz\n",
      "  📄 GSM7757590_CD8_24h_Vehicle3.tabular.txt\n",
      "  📄 GSM7757590_CD8_24h_Vehicle3.tabular.txt.gz\n",
      "  📄 GSM7757591_CD8_24h_PGE2_1.tabular.txt\n",
      "  📄 GSM7757591_CD8_24h_PGE2_1.tabular.txt.gz\n",
      "  📄 GSM7757592_CD8_24h_PGE2_2.tabular.txt\n",
      "  📄 GSM7757592_CD8_24h_PGE2_2.tabular.txt.gz\n",
      "  📄 GSM7757593_CD8_24h_PGE2_3.tabular.txt\n",
      "  📄 GSM7757593_CD8_24h_PGE2_3.tabular.txt.gz\n",
      "  📄 GSM7757594_CD8_48h_Vehicle1.tabular.txt\n",
      "  📄 GSM7757594_CD8_48h_Vehicle1.tabular.txt.gz\n",
      "  📄 GSM7757595_CD8_48h_Vehicle2.tabular.txt\n",
      "  📄 GSM7757595_CD8_48h_Vehicle2.tabular.txt.gz\n",
      "  📄 GSM7757596_CD8_48h_Vehicle3.tabular.txt\n",
      "  📄 GSM7757596_CD8_48h_Vehicle3.tabular.txt.gz\n",
      "  📄 GSM7757597_CD8_48h_PGE2_1.tabular.txt\n",
      "  📄 GSM7757597_CD8_48h_PGE2_1.tabular.txt.gz\n",
      "  📄 GSM7757598_CD8_48h_PGE2_2.tabular.txt\n",
      "  📄 GSM7757598_CD8_48h_PGE2_2.tabular.txt.gz\n",
      "  📄 GSM7757599_CD8_48h_PGE2_3.tabular.txt\n",
      "  📄 GSM7757599_CD8_48h_PGE2_3.tabular.txt.gz\n",
      "  📄 GSM7757600_CD8_60h_Vehicle1.tabular.txt\n",
      "  📄 GSM7757600_CD8_60h_Vehicle1.tabular.txt.gz\n",
      "  📄 GSM7757601_CD8_60h_Vehicle2.tabular.txt\n",
      "  📄 GSM7757601_CD8_60h_Vehicle2.tabular.txt.gz\n",
      "  📄 GSM7757602_CD8_60h_Vehicle3.tabular.txt\n",
      "  📄 GSM7757602_CD8_60h_Vehicle3.tabular.txt.gz\n",
      "  📄 GSM7757603_CD8_60h_PGE2_1.tabular.txt\n",
      "  📄 GSM7757603_CD8_60h_PGE2_1.tabular.txt.gz\n",
      "  📄 GSM7757604_CD8_60h_PGE2_2.tabular.txt\n",
      "  📄 GSM7757604_CD8_60h_PGE2_2.tabular.txt.gz\n",
      "  📄 GSM7757605_CD8_60h_PGE2_3.tabular.txt\n",
      "  📄 GSM7757605_CD8_60h_PGE2_3.tabular.txt.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "gse_path = \"rna_seq_analysis/GSE242272\"\n",
    "for root, dirs, files in os.walk(gse_path):\n",
    "    print(f\"\\n📁 {root}\")\n",
    "    for f in files:\n",
    "        print(f\"  📄 {f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cdb473b-439f-4567-9177-5e022bc6498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import tarfile\n",
    "import gzip\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "# Optional DESeq2 import\n",
    "try:\n",
    "    #from pyDESeq2 import py_DESeq2\n",
    "    from pydeseq2.dds import DeseqDataSet\n",
    "    from pydeseq2.default_inference import DefaultInference\n",
    "    from pydeseq2.ds import DeseqStats\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    use_deseq2 = True\n",
    "except ImportError:\n",
    "    print(\"⚠️ pyDESeq2 not found. Skipping DEG step.\")\n",
    "    use_deseq2 = False\n",
    "\n",
    "\n",
    "def download_and_extract_gse(gse_id=\"GSE242272\", base_dir=\"rna_seq_analysis\"):\n",
    "    download_dir = os.path.join(base_dir, gse_id)\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "    url = f\"https://www.ncbi.nlm.nih.gov/geo/download/?acc={gse_id}&format=file\"\n",
    "    tar_path = os.path.join(download_dir, f\"{gse_id}_supplement.tar\")\n",
    "\n",
    "    print(f\"📦 Downloading: {url}\")\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        with open(tar_path, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        print(f\"✅ Downloaded to: {tar_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Download failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"🗂️ Extracting TAR...\")\n",
    "    try:\n",
    "        with tarfile.open(tar_path, \"r:*\") as tar:\n",
    "            tar.extractall(path=download_dir)\n",
    "        print(f\"✅ Extracted contents to: {download_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Extraction failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    return download_dir\n",
    "\n",
    "\n",
    "def decompress_gz_files(root_dir: str):\n",
    "    print(f\"🔍 Decompressing .txt.gz files...\")\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(\".txt.gz\"):\n",
    "                gz_path = os.path.join(dirpath, filename)\n",
    "                txt_path = gz_path[:-3]\n",
    "                if os.path.exists(txt_path):\n",
    "                    print(f\"⏭️ Skipping: {txt_path} already exists.\")\n",
    "                    continue\n",
    "                try:\n",
    "                    with gzip.open(gz_path, 'rb') as f_in, open(txt_path, 'wb') as f_out:\n",
    "                        shutil.copyfileobj(f_in, f_out)\n",
    "                    print(f\"✅ Decompressed: {gz_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Failed to decompress {gz_path}: {e}\")\n",
    "\n",
    "from pydeseq2.dds import DeseqDataSet\n",
    "from pydeseq2.default_inference import DefaultInference\n",
    "from pydeseq2.ds import DeseqStats\n",
    "def run_deg_analysis_from_gse(gse_id=\"GSE242272\", base_dir=\"rna_seq_analysis\"):\n",
    "    gse_dir = os.path.join(base_dir, gse_id)\n",
    "    output_csv = os.path.join(base_dir, f\"{gse_id}_deseq2_results.csv\")\n",
    "\n",
    "    count_dfs = []\n",
    "    sample_conditions = []\n",
    "\n",
    "    print(f\"\\n🔍 Searching GSM folders in: {gse_dir}\")\n",
    "    for fname in os.listdir(gse_dir):\n",
    "        if fname.endswith(\".txt\") and not fname.endswith(\".txt.gz\"):\n",
    "            file_path = os.path.join(gse_dir, fname)\n",
    "            sample_id = fname.split(\"_\")[0]\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, sep=\"\\t\", header=None, names=[\"gene\", sample_id])\n",
    "                df.set_index(\"gene\", inplace=True)\n",
    "                count_dfs.append(df)\n",
    "\n",
    "                label = \"treated\" if any(x in fname.lower() for x in [\"treated\", \"pge2\", \"caffeine\"]) else \"control\"\n",
    "                sample_conditions.append((sample_id, label))\n",
    "                print(f\"  ✅ Loaded: {fname} as {label}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Failed to read {file_path}: {e}\")\n",
    "\n",
    "    if not count_dfs:\n",
    "        raise ValueError(\"❌ No valid count files found.\")\n",
    "\n",
    "    combined_counts = pd.concat(count_dfs, axis=1).fillna(0).astype(int)\n",
    "\n",
    "    print(f\"\\n✅ Count matrix shape: {combined_counts.shape}\")\n",
    "    print(combined_counts.iloc[:5, :5])\n",
    "\n",
    "    # Save raw counts\n",
    "    raw_csv = os.path.join(base_dir, f\"{gse_id}_raw_counts.csv\")\n",
    "    combined_counts.to_csv(raw_csv)\n",
    "    print(f\"📄 Saved raw count matrix to: {raw_csv}\")\n",
    "\n",
    "    if not use_deseq2:\n",
    "        return combined_counts\n",
    "\n",
    "    # Prepare design metadata\n",
    "    sample_df = pd.DataFrame(sample_conditions, columns=[\"sample\", \"condition\"])\n",
    "    sample_df.set_index(\"sample\", inplace=True)\n",
    "\n",
    "    print(\"\\n🚀 Running DESeq2...\")\n",
    "    dds = py_DESeq2(\n",
    "        count_matrix=combined_counts.T,\n",
    "        design_matrix=sample_df,\n",
    "        design_formula=\"~ condition\",\n",
    "        gene_column=\"gene\"\n",
    "    )\n",
    "    dds.run_deseq()\n",
    "    res = dds.get_deseq_result()\n",
    "    res_sorted = res.sort_values(\"padj\").dropna().head(50)\n",
    "    res_sorted.to_csv(output_csv, index=False)\n",
    "    print(f\"📊 DESeq2 top results saved to: {output_csv}\")\n",
    "\n",
    "    return res_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6352edf-aadf-4316-b4f5-5fff6dc2f611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.5.1'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pydeseq2\n",
    "pydeseq2.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdee6e64-46d8-46d6-a5a6-b68651558877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Downloading: https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE242272&format=file\n",
      "✅ Downloaded to: rna_seq_analysis\\GSE242272\\GSE242272_supplement.tar\n",
      "🗂️ Extracting TAR...\n",
      "✅ Extracted contents to: rna_seq_analysis\\GSE242272\n",
      "🔍 Decompressing .txt.gz files...\n",
      "⏭️ Skipping: rna_seq_analysis\\GSE242272\\GSM7757588_CD8_24h_Vehicle1.tabular.txt already exists.\n",
      "⏭️ Skipping: rna_seq_analysis\\GSE242272\\GSM7757589_CD8_24h_Vehicle2.tabular.txt already exists.\n",
      "⏭️ Skipping: rna_seq_analysis\\GSE242272\\GSM7757590_CD8_24h_Vehicle3.tabular.txt already exists.\n",
      "⏭️ Skipping: rna_seq_analysis\\GSE242272\\GSM7757591_CD8_24h_PGE2_1.tabular.txt already exists.\n",
      "⏭️ Skipping: rna_seq_analysis\\GSE242272\\GSM7757592_CD8_24h_PGE2_2.tabular.txt already exists.\n",
      "⏭️ Skipping: rna_seq_analysis\\GSE242272\\GSM7757593_CD8_24h_PGE2_3.tabular.txt already exists.\n",
      "⏭️ Skipping: rna_seq_analysis\\GSE242272\\GSM7757594_CD8_48h_Vehicle1.tabular.txt already exists.\n",
      "⏭️ Skipping: rna_seq_analysis\\GSE242272\\GSM7757595_CD8_48h_Vehicle2.tabular.txt already exists.\n",
      "⏭️ Skipping: rna_seq_analysis\\GSE242272\\GSM7757596_CD8_48h_Vehicle3.tabular.txt already exists.\n",
      "⏭️ Skipping: rna_seq_analysis\\GSE242272\\GSM7757597_CD8_48h_PGE2_1.tabular.txt already exists.\n",
      "⏭️ Skipping: rna_seq_analysis\\GSE242272\\GSM7757598_CD8_48h_PGE2_2.tabular.txt already exists.\n",
      "⏭️ Skipping: rna_seq_analysis\\GSE242272\\GSM7757599_CD8_48h_PGE2_3.tabular.txt already exists.\n",
      "⏭️ Skipping: rna_seq_analysis\\GSE242272\\GSM7757600_CD8_60h_Vehicle1.tabular.txt already exists.\n",
      "⏭️ Skipping: rna_seq_analysis\\GSE242272\\GSM7757601_CD8_60h_Vehicle2.tabular.txt already exists.\n",
      "⏭️ Skipping: rna_seq_analysis\\GSE242272\\GSM7757602_CD8_60h_Vehicle3.tabular.txt already exists.\n",
      "⏭️ Skipping: rna_seq_analysis\\GSE242272\\GSM7757603_CD8_60h_PGE2_1.tabular.txt already exists.\n",
      "⏭️ Skipping: rna_seq_analysis\\GSE242272\\GSM7757604_CD8_60h_PGE2_2.tabular.txt already exists.\n",
      "⏭️ Skipping: rna_seq_analysis\\GSE242272\\GSM7757605_CD8_60h_PGE2_3.tabular.txt already exists.\n",
      "\n",
      "🔍 Searching GSM folders in: rna_seq_analysis\\GSE242272\n",
      "  ✅ Loaded: GSM7757588_CD8_24h_Vehicle1.tabular.txt as control\n",
      "  ✅ Loaded: GSM7757589_CD8_24h_Vehicle2.tabular.txt as control\n",
      "  ✅ Loaded: GSM7757590_CD8_24h_Vehicle3.tabular.txt as control\n",
      "  ✅ Loaded: GSM7757591_CD8_24h_PGE2_1.tabular.txt as treated\n",
      "  ✅ Loaded: GSM7757592_CD8_24h_PGE2_2.tabular.txt as treated\n",
      "  ✅ Loaded: GSM7757593_CD8_24h_PGE2_3.tabular.txt as treated\n",
      "  ✅ Loaded: GSM7757594_CD8_48h_Vehicle1.tabular.txt as control\n",
      "  ✅ Loaded: GSM7757595_CD8_48h_Vehicle2.tabular.txt as control\n",
      "  ✅ Loaded: GSM7757596_CD8_48h_Vehicle3.tabular.txt as control\n",
      "  ✅ Loaded: GSM7757597_CD8_48h_PGE2_1.tabular.txt as treated\n",
      "  ✅ Loaded: GSM7757598_CD8_48h_PGE2_2.tabular.txt as treated\n",
      "  ✅ Loaded: GSM7757599_CD8_48h_PGE2_3.tabular.txt as treated\n",
      "  ✅ Loaded: GSM7757600_CD8_60h_Vehicle1.tabular.txt as control\n",
      "  ✅ Loaded: GSM7757601_CD8_60h_Vehicle2.tabular.txt as control\n",
      "  ✅ Loaded: GSM7757602_CD8_60h_Vehicle3.tabular.txt as control\n",
      "  ✅ Loaded: GSM7757603_CD8_60h_PGE2_1.tabular.txt as treated\n",
      "  ✅ Loaded: GSM7757604_CD8_60h_PGE2_2.tabular.txt as treated\n",
      "  ✅ Loaded: GSM7757605_CD8_60h_PGE2_3.tabular.txt as treated\n",
      "\n",
      "✅ Count matrix shape: (25239, 18)\n",
      "               GSM7757588  GSM7757589  GSM7757590  GSM7757591  GSM7757592\n",
      "gene                                                                     \n",
      "0610005C13Rik           0           0           1           0           0\n",
      "0610009B22Rik         295         252         267         231         225\n",
      "0610009E02Rik          47          53          48          67          30\n",
      "0610009L18Rik           5          13          11          10          14\n",
      "0610010F05Rik         249         332         348         273         324\n",
      "📄 Saved raw count matrix to: rna_seq_analysis\\GSE242272_raw_counts.csv\n",
      "\n",
      "🚀 Running DESeq2...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'py_DESeq2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m     decompress_gz_files(path)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Step 3: Assemble and analyze\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mrun_deg_analysis_from_gse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgse_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 116\u001b[0m, in \u001b[0;36mrun_deg_analysis_from_gse\u001b[1;34m(gse_id, base_dir)\u001b[0m\n\u001b[0;32m    113\u001b[0m sample_df\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample\u001b[39m\u001b[38;5;124m\"\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m🚀 Running DESeq2...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 116\u001b[0m dds \u001b[38;5;241m=\u001b[39m \u001b[43mpy_DESeq2\u001b[49m(\n\u001b[0;32m    117\u001b[0m     count_matrix\u001b[38;5;241m=\u001b[39mcombined_counts\u001b[38;5;241m.\u001b[39mT,\n\u001b[0;32m    118\u001b[0m     design_matrix\u001b[38;5;241m=\u001b[39msample_df,\n\u001b[0;32m    119\u001b[0m     design_formula\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m~ condition\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    120\u001b[0m     gene_column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgene\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    121\u001b[0m )\n\u001b[0;32m    122\u001b[0m dds\u001b[38;5;241m.\u001b[39mrun_deseq()\n\u001b[0;32m    123\u001b[0m res \u001b[38;5;241m=\u001b[39m dds\u001b[38;5;241m.\u001b[39mget_deseq_result()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'py_DESeq2' is not defined"
     ]
    }
   ],
   "source": [
    "gse_id = \"GSE242272\"\n",
    "base_dir = \"rna_seq_analysis\"\n",
    "\n",
    "# Step 1: Download and extract\n",
    "path = download_and_extract_gse(gse_id, base_dir)\n",
    "\n",
    "# Step 2: Decompress only missing .txt files\n",
    "if path:\n",
    "    decompress_gz_files(path)\n",
    "\n",
    "# Step 3: Assemble and analyze\n",
    "result = run_deg_analysis_from_gse(gse_id, base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dbffbc-b87c-4734-8a49-863dcd4e6e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydeseq2.dds import DeseqDataSet\n",
    "from pydeseq2.default_inference import DefaultInference\n",
    "from pydeseq2.ds import DeseqStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99bb1e0b-f1c4-436e-90de-c5437fc7199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated import using correct PyDESeq2 structure\n",
    "from pydeseq2.dds import DeseqDataSet\n",
    "from pydeseq2.default_inference import DefaultInference\n",
    "from pydeseq2.ds import DeseqStats\n",
    "\n",
    "def run_deseq2_manual(counts_df, metadata_df, contrast=[\"condition\", \"treated\", \"control\"], output_path=\"rna_seq_analysis\"):\n",
    "    # Filter out samples with missing conditions\n",
    "    print(\"📋 Initial metadata:\")\n",
    "    print(metadata_df)\n",
    "\n",
    "    samples_to_keep = ~metadata_df.condition.isna()\n",
    "    counts_df = counts_df.loc[samples_to_keep]\n",
    "    metadata_df = metadata_df.loc[samples_to_keep]\n",
    "\n",
    "    # Filter genes with low counts\n",
    "    genes_to_keep = counts_df.columns[counts_df.sum(axis=0) >= 10]\n",
    "    counts_df = counts_df[genes_to_keep]\n",
    "\n",
    "    # Set up DESeq2\n",
    "    inference = DefaultInference(n_cpus=1)\n",
    "    dds = DeseqDataSet(\n",
    "        counts=counts_df,\n",
    "        metadata=metadata_df,\n",
    "        design_factors=\"condition\",\n",
    "        refit_cooks=True,\n",
    "        inference=inference,\n",
    "    )\n",
    "\n",
    "    dds.deseq2()\n",
    "    print(\"📊 LFC matrix:\")\n",
    "    print(dds.varm[\"LFC\"].head())\n",
    "\n",
    "    # Run differential expression stats\n",
    "    ds = DeseqStats(dds, contrast=contrast, inference=inference)\n",
    "    ds.summary()\n",
    "\n",
    "    results_path = os.path.join(output_path, \"results.csv\")\n",
    "    ds.results_df.to_csv(results_path)\n",
    "    print(f\"✅ DESeq2 results saved to: {results_path}\")\n",
    "\n",
    "    return ds.results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06a8a0e-553d-4e97-b19c-2b685058993c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Initial metadata:\n",
      "              condition\n",
      "0610005C13Rik   control\n",
      "0610009B22Rik   control\n",
      "0610009E02Rik   control\n",
      "0610009L18Rik   control\n",
      "0610010F05Rik   control\n",
      "...                 ...\n",
      "Zyx             control\n",
      "Zzef1           control\n",
      "Zzz3            control\n",
      "a               control\n",
      "ccdc198         control\n",
      "\n",
      "[25239 rows x 1 columns]\n",
      "Using None as control genes, passed at DeseqDataSet initialization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\difen\\AppData\\Local\\Temp\\ipykernel_30136\\774074505.py:21: DeprecationWarning: design_factors is deprecated and will soon be removed.Please consider providing a formulaic formula using the design argumentinstead.\n",
      "  dds = DeseqDataSet(\n",
      "Fitting size factors...\n",
      "C:\\Users\\difen\\venv1\\Lib\\site-packages\\pydeseq2\\dds.py:533: UserWarning: Every gene contains at least one zero, cannot compute log geometric means. Switching to iterative mode.\n",
      "  self.fit_size_factors(\n",
      "Fitting dispersions...\n",
      "... done in 2.20 seconds.\n",
      "\n",
      "Fitting MAP dispersions...\n",
      "... done in 1.44 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load real data saved earlier\n",
    "gse_id = \"GSE242272\"\n",
    "base_dir = \"rna_seq_analysis\"\n",
    "counts_path = os.path.join(base_dir, f\"{gse_id}_raw_counts.csv\")\n",
    "metadata_path = os.path.join(base_dir, f\"{gse_id}_sample_metadata.csv\")\n",
    "\n",
    "# Read count matrix\n",
    "counts_df = pd.read_csv(counts_path, index_col=0)\n",
    "\n",
    "# Read or reconstruct metadata (if not saved separately, recreate it from column names)\n",
    "sample_names = counts_df.index.tolist()\n",
    "conditions = [\"treated\" if any(x in name.lower() for x in [\"pge2\", \"caffeine\", \"treated\"])\n",
    "              else \"control\" for name in sample_names]\n",
    "metadata_df = pd.DataFrame({\"condition\": conditions}, index=sample_names)\n",
    "\n",
    "# Run DESeq2 analysis on real data\n",
    "results_df = run_deseq2_manual(counts_df, metadata_df, contrast=[\"condition\", \"treated\", \"control\"], output_path=base_dir)\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17f3472-910f-48bd-8768-c1078ff6cb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e82181-4b74-473a-9472-0294d087d48c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "venv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
